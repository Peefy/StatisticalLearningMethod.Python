
class Chapter12:
    """
    第12章 统计学习方法总结
    """
    def __init__(self):
        """
        第12章 统计学习方法总结
        """
        pass

    def note(self):
        """
        chapter12 note
        """
        print('第12章 统计学习方法总结')
        print('10种主要的统计学习方法:感知机、k近邻法、朴素贝叶斯法、决策树、',
            '逻辑斯谛回归与最大熵模型、支持向量机、提升方法、EM算法、隐马尔可夫模型和条件随机场')
        print('1.适用问题')
        print('本书主要介绍监督学习方法.监督学习可以认为是学习一个模型,使它能对给定的输入预测相应的输出.',
            '监督学习包括分类、标注、回归.')
        print('分类问题是从实例的特征向量到类标记的预测问题,标注问题是从观测序列到标记序列(或状态序列)的预测问题.',
            '可以认为分类问题是标注问题的特殊情况.分类问题中可能的预测结果是二类或多类.',
            '而标注问题中可能的预测结果是所有的标记序列,其数目是指数级的.')
        print('感知机、k近邻法、朴素贝叶斯法、决策树、逻辑斯谛回归与最大熵模型、',
            '支持向量机、提升方法是分类方法.原始的感知机、支持向量机以及提升方法是针对二类分类的,',
            '可以将它们扩展到多类分类的。隐马尔可夫模型、条件随机场是标注方法.EM算法是含有隐变量的概率模型的一般学习算法,',
            '可以用于生成模型的非监督学习.')
        print('感知机、k近邻法、朴素贝叶斯法、决策树是简单的分类方法,具有模型直观、方法简单、实现容易等特点.',
            '逻辑斯谛回归与最大熵模型、支持向量机、提升方法是更复杂但更有效的分类方法，',
            '往往分类准确率更高。隐马尔可夫模型、条件随机场是主要的标注方法.通常条件随机场的标注准确率更高.')
        print('2.模型')
        print('分类问题与标注问题的预测模型都可以认为是表示从输入空间到输出空间的映射.它们可以写成条件概率分布P(Y|X)',
            '或决策函数Y=f(X)的形式.前者表示给定输入条件下输出的概率模型,后者表示输入到输出的非概率模型.',
            '有时,模型更直接地表示为概率模型,或者非概率模型;但有时模型兼有两种解释.')
        print('朴素贝叶斯法、隐马尔可夫模型是概率模型.感知机、k近邻法、支持向量机、提升方法是非概率模型.',
            '而决策树、逻辑斯谛回归与最大熵模型、条件随机场既可以看做是概率模型,又可以看做是非概率模型.')
        print('直接学习条件概率分布P(Y|X)或决策函数Y=f(X)的方法为判别方法,对应的模型是判别模型.',
            '感知机、k近邻法、决策树、逻辑斯谛回归与最大熵模型、支持向量机、提升方法、条件随机场是判别方法,',
            '首先学习联合概率分布P(X,Y),从而求得条件概率分布P(Y|X)的方法是生成方法,对应的模型是生成模型.',
            '朴素贝叶斯法、隐马尔可夫模型是生成方法')
        print('可以用非监督学习的方法学习生成模型.具体地,应用EM算法可以学习朴素贝叶斯模型以及隐马尔可夫模型')
        print('决策树是定义在一般的特征空间上的,可以含有连续变量或离散变量.感知机、支持向量机、k近邻法的特征空间是欧式空间',
            '(可以含有连续变量或离散变量.感知机、支持向量机、k近邻法的特征空间是欧式空间)')
        print('感知机模型是线性模型,而逻辑斯谛回归与最大熵模型、条件随机场是对数线性模型.k近邻法、决策树',
            '支持向量机(包含核函数)、提升方法使用的是非线性模型.')
        print('3.学习策略')
        print('在二类分类的监督学习中,支持向量机、逻辑斯谛回归与最大熵模型、提升方法各自使用合页损失函数、',
            '逻辑斯谛损失函数、指数损失函数.3中损失函数分别写为：',
            '[1-yf(x)]+',
            'log[1+exp(-yf(x))]',
            'exp(-yf(x))')
        print('这3种损失函数都是0-1损失函数的上界,具有相似的形状,所以,',
            '可以认为支持向量机、逻辑斯谛回归与最大熵模型、提升方法使用不同的代理损失函数表示分类的损失,',
            '定义经验风险或结构风险损失函数,实现二类分类学习任务.学习的策略是优化以下结构风险函数：',
            'min1/N∑L(yi,f(xi))+lJ(f)')
        print('这里,第1项为经验风险（经验损失）,第2项为正则化项,L(y,f(x))为损失函数,',
            'J(f)为模型的复杂度,l>=0为系数.')
        print('支持向量机用L2范数表示模型的复杂度.原始的逻辑斯谛回归与最大熵模型没有正则化项,',
            '可以给它们加上L2范数正则化项.提升方法没有显式的正则化项,',
            '通常通过早停止的方法达到正则化的效果.')
        print('以上二类分类的学习方法可以扩展到多类分类学习以及标注问题,',
            '比如标注问题的条件随机场可以看作是分类问题的最大熵模型的推广.')
        print('概率模型的学习可以形式化为极大似然估计或贝叶斯估计的极大后验概率估计.',
            '这时,学习的策略是极小化对数似然损失或极小化正则化的对数似然损失.',
            '对数似然可以写成-logP(y|x)')
        print('极大后验概率估计时,正则化项是先验概率的负对数.')
        print('决策树学习的策略是正则化的极大似然估计,损失函数是对数似然损失,',
            '正则化项是决策树的复杂度.')
        print('逻辑斯谛回归与最大熵模型、条件随机场的学习策略既可以看成是极大似然估计',
            '(或正则化的极大似然估计),又看成是极小化逻辑斯谛损失(或正则化的逻辑斯谛损失)')
        print('朴素贝叶斯模型、隐马尔可夫模型的非监督学习也是极大似然估计或极大后验概率估计,',
            '但这时模型含有隐变量')
        print('4.学习算法')
        print('统计学习的问题有了具体的形式以后,就变成了最优化问题.有时,最优化问题比较简单,',
            '解析解存在,最优解可以由公式简单计算.但在多数情况下,最优化问题没有解析解,',
            '需要用数值计算的方法或启发式的方法求解.')
        print('朴素贝叶斯法与隐马尔可夫模型的监督学习,最优解即极大似然估计值,',
            '可以由概率计算公式直接计算.')
        print('感知机、逻辑斯谛回归与最大熵模型、条件随机场的学习利用梯度下降法、',
            '拟牛顿法等.这些都是一般的无约束最优化问题的解法.')
        print('支持向量机学习,可以解凸二次规划的对偶问题.有序列最小最优化算法等方法.')
        print('决策树学习是基于启发式算法的典型例子.可以认为特征选择、生成、',
            '剪枝是启发式地进行正则化的极大似然估计.')
        print('提升方法利用学习的模型是加法模型、损失函数是指数损失函数的特点,',
            '启发式地从前向后逐步学习模型,以达到逼近优化目标函数的目的.')
        print('EM算法是一种迭代的求解含隐变量概率模型参数的方法,',
            '它的收敛性可以保证,但是不能保证收敛到全局最优.')
        print('支持向量机学习、逻辑斯谛回归与最大熵模型学习、条件随机场学习是凸优化问题,',
            '全局最优解保证存在.而其他学习问题则不是凸优化问题.')

chapter12 = Chapter12()

def main():
    chapter12.note()

if __name__ == '__main__':
    main()
