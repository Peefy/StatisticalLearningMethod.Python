
class Chapter2:
    """
    第2章 感知机
    """
    def __init__(self):
        """
        第2章 感知机
        """
        pass

    def note(self):
        """
        chapter2 note
        """
        print('第2章 感知机')
        print('感知机(perception)是二类分类的线性分类模型,其输入为实例的特征向量,输出为实例的类别,取+1和-1二值')
        print('感知机对应于输入空间(特征空间)中将实例划分为正负两类的分离超平面,属于判别模型.感知机学习旨在求出将训练数据进行线性划分的分离超平面')
        print('为此,导入基于误分类的损失函数,利用梯度下降法对损失函数进行极小化,求得感知机模型.')
        print('感知机学习算法具有简单而易于实现的优点,分为原始形式和对偶形式.感知机预测是用学习得到的感知机模型对新的输入实例进行分类.')
        print('感知机1957年由Rosenblatt提出,是神经网络与支持向量机的基础.')
        print('本章介绍感知机模型、学习策略(损失函数)、学习算法(原始形式，对偶形式),并证明算法的收敛性')
        print('2.1 感知机模型')
        print('定义2.1 (感知机) 假设输入空间(特征空间)是X∈R^n,输出空间是Y={+1,-1}.',
            '输入x∈X表示实例的特征向量,对应于输入空间(特征空间)的点；输出y∈Y表示实例的类别.',
            '由输入空间到输出空间的如下函数:f(x)=sign(w·x+b)称为感知机',
            '其中,w和b为感知机模型参数,w∈R^n叫做权值(weight)或权值向量,b∈R叫作偏置,w·x表示w和x的内积.sign是符号函数')
        print('sign(x)=1,x>=0; sign(x)=-1,x<0')
        print('感知机是一种线性分类模型,属于判别模型.感知机模型的假设空间是定义在特征空间中的所有线性分类模型或者线性分类器,',
            '即函数集合{f|f(x)=w·x+b}')
        print('感知机有如下几何解释:线性方程w·x+b=0对应于特征空间R^n中的一个超平面S,其中w是超平面的法向量,b是超平面的截距.')
        print('这个超平面将特征空间划分为两个部分.位于两部分的点(特征向量)分别被正、负两类.因此,超平面S称为分离超平面')
        print('感知机学习,由训练数据集(实例的特征向量及类别).其中,xi∈X=R^n,yi∈Y={+1,-1},i=1,2,...,N,求得感知机模型,',
            '即求得模型参数w,b.感知机预测,通过学习得到的感知机模型,对于信的输入实例给出其对应的输出类别')
        print('2.2 感知机学习策略')
        print('2.2.1 数据集的线性可分性')
        print('定义2.2 (数据集的线性可分性) 给定一个数据集T={(x1,y1),(x2,y2),...,(xn,yn)}')
        print('其中,xi∈X=R^n,yi∈Y={+1,-1},i=1,2,...,N,如果存在某个超平面S,w·x+b=0能够将数据集的正实例点和负实例点完全正确地划分到超平面的两侧,',
            '即对所有yi=+1的实例i,有w·xi+b>0,对所有yi=-1的实例i,有w·xi+b<0,则称数据集T为线性可分数据集;否则,称数据集T线性不可分')
        print('2.2.2 感知机学习策略')
        print('假设训练数据集是线性可分的,感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面.为了找出这样的超平面,',
            '即确定感知机模型参数w,b,需要确定一个学习策略,即定义(经验)损失函数并将损失函数极小化')
        print('损失函数的一个自然选择是误分类点的总数.但是,这样的损失函数不是参数w,b的连续可导函数,不易优化.',
            '损失函数的另一个选择是误分类点到超平面S的总距离,这是感知机所采用的.为此,首先写出输入空间R^n中任一点x0到超平面S的距离:',
            '1/||w|||w·x0+b|')
        print('这里,||w||是w的L2范数')
        print('其次,对于误分类的数据(xi,yi)来说:-yi(w·xi+b)>0')
        print('成立.因此当w·xi+b>0时,yi=-1,而当w·xi+b<0时,yi=+1.因此,误分类点xi到超平面S的距离-1/||w||yi(w·xi+b)')
        print('这样,假设超平面S的误差分类点集合为M,那么所有误分类点到超平面S的总距离为：-1/||w||∑yi(w·xi+b)')
        print('不考虑1/||w||,就得到感知机学习的损失函数')
        print('给定训练数据集:T={(x1,y1),(x2,y2),...,(xN,yN)}')
        print('其中,xi∈X=R^n,yi∈Y={+1,-1},i=1,2,...,N.感知机sign(w·x+b)学习的损失函数定义为L(w,b)=-∑yi(w·xi+b)',
            '其中M为误分类点的集合.这个损失函数就是感知机学习的经验风险函数')
        print('显然,损失函数L(w,b)是非负的.如果没有误分类点,损失函数值是0.而且,误分类点越少,误分类点离超平面越近,损失函数值就越小.',
            '一个特定的样本点的损失函数:在误分类时是参数w,b的线性函数,在正确分类时是0.因此,给定训练数据集T,损失函数L(w,b)是w,b的连续可导函数')
        print('感知机学习的策略是在假设空间中选取使损失函数最小的模型参数w,b,即感知机模型.')
        print('2.3 感知机学习算法')
        print('2.3.1 感知机学习算法的原始形式')
        print('感知机学习算法是对以下最优化问题的算法.给定一个训练数据集T={(x1,y1),(x2,y2),...,(xN,yN)}.',
            '其中,xi∈X=R^n,yi∈Y={-1,1},i=1,2,...,N,求参数w,b,使其为以下损失函数极小化问题的解:',
            'minL(w,b)=-∑yi(w·xi+b),其中M为误分类点的集合')
        print('感知机学习算法是误分类驱动的,具体采用随机梯度下降算法.首先,任意选取一个超平面w0,b0,然后用梯度下降法不断地极小化目标函数',
            '极小化过程中不是一次使M中所有误分类点的梯度下降,而是一次随机选取一个误分类点使其梯度下降.')
        print('假设误分类点集合M是固定的,那么损失函数L(w,b)的梯度公式为:',
            '∇wL(w,b)=-∑yixi;  ∇bL(w,b)=-∑yi;给出')
        print('随机选取一个误分类点(xi,yi),对wib进行更新w=w+ηyixi; b=b+ηyi')
        print('式中η(0<η<=1)是步长,在统计学习中又称为学习率(learning rate).这样,通过迭代可以期待损失函数L(w,b)不断减小,直到为0.')
        print('综上所述,得到如下算法:')
        print('算法2.1 (感知机学习算法的原始形式)')
        print('输入:训练数据集T={(x1,y1),(x2,y2),...,(xN,yN)},其中xi∈X=R^n,yi=Y={-1,1},i=1,2,...,N;学习率η(0<η<=1),也是步长')
        print('输出:w,b;感知机模型f(x)=sign(w·x+b)')
        print('(1) 选取初值w0,b0')
        print('(2) 在训练集中选取数据(xi,yi)')
        print('(3) 如果yi(w·xi+b)<=0;   w<-w+ηyixi;  b<-b+ηyi')
        print('(4) 转至(2),直至训练集中没有误分类点. ')
        print('感知机学习方法的直观理解:')
        print('    当一个实例点被误分类,即位于分离超平面的错误一侧时,则调整w,b的值,使分离超平面向该误分类点的一侧移动,',
            '以减少该误分类点与超平面见的距离,直至超平面越过该误分类点使其被正确分类')
        print('    算法2.1 是感知机学习的基本算法,对应于后面的对偶形式,称为原始形式.感知机学习算法简单且易于实现.')
        print('例2.1 如图2.2所示的训练数据集.其正实例点是x1=(3,3)^T,x2=(4,3)^T,负实例点是x3=(1,1)^T.',
            '试用感知机学习算法的原始形式求感知机模型f(x)=sign(w·x+b).这里,w=(w(1),w(2))^T,x=(x(1),x(2))^T.')
        print('  解:构建最优化问题:minL(w,b)=-∑yi(w·x+b),按照算法2.1求解w,b. η=1.')
        print('  (1) 取初值w0=0,b0=0')
        print('  (2) 对x1=(3,3)^T, y1(w0·x1+b0)=0, 未能被正确分类,更新w,b:')
        print('           w1=w0+y1x1=(3,3)^T,b1=b0+y1=1')
        print('  得到线性模型w1·x+b1=3x(1)+3x(2)+1')
        print('  (3) 对x1,x2,显然,yi(w1·xi+b1)>0,被正确分类,不修改w,b;')
        print('  对x3=(1,1)^T,y3(w1·x3+b1)<0,被误分类,更新w,b')
        print('            w2=w1+y3x3=(2,2)^T,  b2=b1+y3=0')
        print('  得到线性模型:w2·x+b2=2x(1)+2x(2)')
        print('  如此继续下去,直到w7=(1,1)^T, b7=-3   w7·x+b7=x(1)+x(2)-3')
        print('  对所有数据点yi(w7·xi+b7)>0,没有误分类点,损失函数达到极小')
        print('  分离超平面为:x(1)+x(2)-3=0')
        print('  感知机模型为:f(x)=sign(x(1)+x(2)-3)')
        print('  如果在计算中误分类点先后取x1,x3,x3,x3,x1,x3,x3得到的超平面和感知机模型.')
        print('  如果在计算中误分类点依次取x1,x3,x3,x3,x2,x3,x3,x3,x1,x3,x3,那么得到的分离超平面是2x(1)+x(2)-5=0')
        print('可见,感知机学习算法由于采用不同的初值或选取不同的误分类点,解可以不同.')
        print('2.3.2 算法的收敛性')
        print('现在证明,对于线性可分数据集感知机学习算法原始形式收敛,',
            '即经过有限次迭代可以得到一个将训练数据集完全正确划分的分离超平面及感知机模型')
        print('为了便于叙述与推导,将偏置b并入权重向量w,记作w’=(w^T,b)^T,同样也将输入向量加以扩充,加进常数1,记作x‘=(x^T,1)^T.',
            '这样,x∈R^(n+1),w∈R^(n+1).显然,w‘·x’=w·x+b')
        print('定理2.1 (Novikoff) 设训练数据集T={(x1,y1),(x2,y2),..,(xN,yN)}是线性可分的,其中xi∈X=R^n,yi=Y={-1,+1},i=1,2,...,N,则',
            '(1) 存在满足条件||w‘opt||的超平面w‘opt·x‘=yi(wopt·xi+bopt)>=y')
        print('(2) 令R=max||xi||,则感知机算法2.1在训练数据集上的误分类次数k满足不等式k<=(R/y)^2')
        print('2.3.3 感知机学习算法的对偶形式')
        print('对偶形式的基本想法是,将w和b表示为实例xi和标记yi的线性组合的形式,通过求解其系数而求得w和b.不失一般性,',
            '在算法2.1中可假设初始值w0,b0均为0.对误分类点(xi,yi)通过w<-w+ηyixi; b<-b+ηyi')
        print('逐步修改w,b,设修改n次,则w,b关于(xi,yi)的增量分别是αiyixi和αiyi,这里αi=niη.')
        print('从学习的过程不难看出,最后学习到的w,b可以分别表示为:') 
        print('w=∑αiyixi; b=∑αiyi.  这里αi>=0,i=1,2,...,N,当η=1时,表示第i个实例点由于误分而进行更新的次数.',
            '实例点更新的次数越多,意味着它距离分离超平面越近,也就越难正确分类.换句话说,这样的实例对学习的结果影响最大.')
        print('算法2.2 (感知机学习算法的对偶形式)')
        print('  输入:线性可分的数据集T={(x1,y1),(x2,y2),..,(xN,yN)},其中xi∈R^n,yi∈{-1,+1},i=1,2,...,N;学习率η(0<η<=1);')
        print('  输出:α,b;感知机模型f(x)=sign(∑αjyjxj·x+b)')
        print('  其中α=(α1,α2,...,αN)^T')
        print('  (1) α<-0,b<-0')
        print('  (2) 在训练集中选取数据(xi,yi)')
        print('  (3) 如果yi(∑αjyjxj·xi+b)))<=0  αi<-αi+η  b<-b+ηyi')
        print('  (4) 转至(2)直到没有误分类数据')
        print('例2.2 数据同例2.1,正样本点是x1=(3,3)^T,x2=(4,3)^T,负样本点是x3=(1,1)^T,',
            '试用感知机学习算法对偶形式求感知机模型')
        print('解 按照算法2.2')
        print(' (1) 取αi=0,i=1,2,3,b=0,η=1')
        print(' (2) 计算Gram矩阵G=[[18,21,6],[21,25,7],[6,7,2]]')
        print(' (3) 误分条件yi(∑αjyjxj·xi+b)<=0 参数更新αi<-αi+1, b=b+yi')
        print(' (4) 迭代.过程从略,结果列于表2.2')
        print(' (5) w=2x1+0x2-5x3=(1,1)^T, b=-3')
        print(' 分离超平面x(1)+x(2)-3=0')
        print(' 感知机模型f(x)=sign(x(1)+x(2)-3)')
        print('与原始形式一样,感知机学习算法的对偶形式迭代是收敛的,存在多个解')
        print('本章概要')
        print('1.感知机是根据输入实例的特征向量x对其进行二类分类的线性分类模型:f(x)=sign(w·x+b)',
            '感知机模型对应于输入空间(特征空间)中的分离超平面')
        print('2.感知机学习的策略是极小化损失函数：minL(w,b)=-∑yi(w·xi+b)',
            'minL(w,b)=-∑yi(w·xi+b). 损失函数对应于误分类点到分离超平面的总距离.')
        print('3.感知机学习算法是基于随机梯度下降法的对损失函数的最优化算法,',
            '有原始形式和对偶形式.算法简单且易于实现.原始形式中,首先任意选取一个超平面,然后用梯度下降法不断极小化目标函数',
            '在这个过程中一次随机选取一个误分类点使其梯度下降.')
        print('4.当训练数据集线性可分时,感知机学习算法是收敛的.感知机算法在训练数据集上的误分类次数k满足不等式：k<=(R/y)^2')
        print('当训练数据集线性可分时,感知机学习算法存在无穷多个解,其解由于不同的初值或不同的迭代顺序而有可能有所不同.')
        print('习题')
        print('习题2.1 感知机因为是线性模型,所以不能表示复杂的函数,如异或函数(XOR).',
            '其实不光感知机无法处理异或问题，所有的线性分类模型都无法处理异或分类问题')
        print('异或问题的输入输出对应关系为')
        print('x1 x2 y')
        print('0 0 0')
        print('0 1 1')
        print('1 0 1')
        print('1 1 0')
        print('这在二维平面中表示(0, 0),0 (1, 1)0, (1, 0)1, (0, 1)1这四个点,所以无论如何都无法找到这样一条线性的直线将这四个点分开')
        print('习题2.2 略')
        print('习题2.3 证明:样本集线性可分的充分必要条件是正实例点集所构成的凸壳与负实例点集所构成的凸壳互不相交')

chapter2 = Chapter2()

def main():
    chapter2.note()

if __name__ == '__main__':
    main()