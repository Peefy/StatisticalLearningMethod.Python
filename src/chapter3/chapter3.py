
class Chapter3:
    """
    第3章 k近邻法
    """
    def __init__(self):
        """
        第3章 k近邻法
        """
        pass

    def note(self):
        """
        chapter3 note
        """
        print('第3章 k近邻法')
        print('k近邻法(k-nearset neighbor, k-NN)是一种基本分类方法与回归方法.本章只讨论分类问题中的k近邻法.',
            'k近邻法的输入为实例的特征向量,对应于特征空间的点;输出为实例的类别,可以取多类.')
        print('k近邻法假定给定一个训练数据集,其中的实例类别已定.分类时,对新的实例,根据其k个最邻近的训练实例的类别,',
            '通过多数表决等方式进行预测.')
        print('因此,k近邻法不具有显式的学习过程.k近邻法实际上利用训练数据集对特征向量空间进行划分,并作为其分类的“模型”.',
            'k值的选择、距离度量及分类决策规则是k近邻法的三个基本要素.k近邻法1968年由Cover和Hart提出')
        print('k近邻法模型、三个基本要素、k近邻法的一个实现方法----kd树,构造kd树和搜索kd树的算法')
        print('3.1 k近邻算法')
        print('k近邻算法简单、直观：给定一个训练数据集,对新的输入实例,在训练数据集中找到与该实例最邻近的k个实例,',
            '这k个实例的多数属于某个类,就把该输入实例分为这个类.')
        print('算法3.1 (k近邻算法)')
        print('  输入:训练数据集T={(x1,y1),(x2,y2),...,(xn,yn)}')
        print('  其中,xi∈X∈R^n为实例的特征向量,yi∈Y={c1,c2,...,cK}为实例的类别,i=1,2,...,N;实例特征向量x')
        print('  输出:实例x所属的类y.')
        print('  (1) 在Nk(x)中根据分类决策规则(如多数表决)决定x的类别y:')
        print('  y=argmax∑I(yi=cj),i=1,2,...,N; j=1,2,...,K')
        print('  I为指示函数,即当yi=cj时I为1,否则I为0.')
        print('k近邻法的特殊情况是k=1的情形,称为最近邻算法.对于输入的实例点(特征向量)x,最近邻法将训练数据集中与x最邻近点的类作为x的类.')
        print('3.2 k近邻模型')
        print('k近邻法使用的模型实际上对应于特征空间的划分.模型由三个基本要素----距离度量、k值的选择和分类决策规则决定.')
        print('3.2.1 模型')
        print('k近邻法中,当训练集、距离度量(如欧式距离)、k值及分类决策规则(如多数表决)确定后,对于任何一个新的输入实例,',
            '它所属的类唯一地确定.这相当于根据上述要素将特征空间划分为一些子空间,确定子空间里的每个点所属的类')
        print('3.2.2 距离度量')
        print('特征空间中两个实例点的距离是两个实例点相似程度的反映.k近邻模型的特征空间一般是n维实数向量空间R^n.',
            '使用的距离是欧式距离,但也可以是其他距离,如更一般的Lp距离或Minkowski距离.(类似于向量的L范数)')
        print('设特征空间X是n维实数向量空间R^n,点xi和点xj的Lp距离为：')
        print('Lp(xi,xj)=(∑|xi-xj|^p)^(1/p); p>=1')
        print('当p=2时,称为欧式距离,当p=1时,称为欧式距离.')
        print('当p=∞时,它是各个坐标距离的最大值,即L∞(xi,xj)=max|xi(l)-xj(l)|')
        print('p=1时在平面上表示一个正菱形,p=2在平面上表示一个圆,p=∞在平面上表示一个正方形')
        print('例3.1 已知二维空间的3个点x1=(1,1)^T, x2=(5,1)^T, x3=(4,4)^T, 试求在p取不同值时,Lp距离下x1的最近邻点')
        print('解: 因为x1和x2只有第二维上值不同,所以p为任何值时,Lp(x1,x2)=4.而:')
        print('  L1(x1,x3)=6, L2(x1,x3)=4.24, L3(x1,x3)=3.78, L4(x1,x3)=3.57')
        print('于是得到:p等于1或2时,x2是x1的最近邻点;p大于等于3时,x3是x1的最近邻点.')
        print('3.2.3 k值的选择')
        print('k值的选择会对k近邻法的结果产生重大影响.')
        print('如果选择较小的k值,就相当于用较小的邻域中的训练实例进行预测,“学习”的近似误差会减小,只有与输入实例较近的(相似的)',
            '训练实例才会对预测结果起作用.','但缺点是“学习”的估计误差会增大,预测结果会对近邻的实例点非常敏感.')
        print('如果邻近的实例点恰好是噪声,预测就会出错.换句话说,k值的减小就意味着整体模型变得复杂,容易发生过拟合')
        print('如果选择较大的k值,就相当于用较大邻域中的训练实例进行预测.其优点是可以减少学习的估计误差.',
            '但缺点是学习的近似误差会增大.这时与输入实例较远的(不相似的)训练实例也会起预测作用,使预测发生错误.',
            'k值的增大就意味着整体的模型变得简单')
        print('如果k=N,那么无论输入实例是什么,都将简单地预测它属于在训练实例中最多的类.',
            '这时,模型过于简单,完全忽略训练实例中的大量有用信息,是不可取的')
        print('在应用中,k值一般取一个比较小的数值.通常采用交叉验证法来选取最优的k值')
        print('3.2.4 分类决策规则')
        print('k近邻法中的分类决策规则往往是多数表决的,即由输入实例的k个邻近的训练实例中的多数类决定输入实例的类')
        print('多数表决规则有如下解释:如果分类的损失函数为0-1损失函数,分类函数为f:R^n->{c1,c2,...,cK}')
        print('那么误分类的概率是:P(Y!=f(X))=1-P(Y=f(X))')
        print('对给定的实例x∈X,其最近邻的k个训练实例点构成集合Nk(x).如果涵盖Nk(x)的区域的类别是cj,',
            '那么误分类率是1/k∑I(yi!=cj)=1-1/k∑I(yi=cj)')
        print('要使误分类率最小即经验风险最小,就要使∑I(yi=cj)最大.所以多数表决规则等价于经验风险最小化.')
        print('3.3 k近邻法的实现：kd树')
        print('实现k近邻法时,主要考虑的问题是如何对训练数据进行快速k近邻搜索.这点在特征空间的维数大及训练数据容量大时尤其必要.')
        print('k近邻法最简单的实现方法是线性扫描.这时要计算输入实例与每一个训练实例的距离.当训练集很大时,计算非常耗时')
        print('3.3.1 构造kd树')
        #! kd树的构造过程类似于并归递归排序
        print('kd树是一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构.kd树是二叉树,表示对k维空间的一个划分(partition)',
            '构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间划分,构成一系列的k维超矩形区域.kd树的每个节点对应于一个k维超矩形区域')
        print('构造kd树的方法如下:构造根结点,使根结点对应于k维空间中包含所有实例点的超矩形区域,通过递归方法,',
            '不断地对k维空间进行划分,生成子结点.在超矩形区域(结点)上选择一个坐标轴和在此坐标轴上的一个切分点,',
            '确定一个超平面,这个超平面通过选定的切分点并垂直于选定的坐标轴,将当前超矩形区域切分为左右两个子区域(子结点);',
            '这时,实例被分到两个子区域.这个过程直到子区域内没有实例时终止(终止时的结点为叶结点).在此过程中,将实例保存在相应的结点上')
        print('注意：平衡的kd树搜索时的效率未必是最优的')
        print('算法3.2 (构造平衡kd树)')
        print('输入:k维空间数据集T={x1,x2,...,xN}')
        print('其中,xi=(xi(1),xi(2),...,xi(k)),i=1,2,...,N')
        print('输出:kd树')
        print('  (1) 开始:构造根结点,跟结点对应于包含T的k维空间的超矩形区域')
        print('  选择x(1)为坐标轴,以T中所有的实例的x(1)坐标的中位数为切分点,将根结点对应的超矩形区域切分为两个子区域.',
            '切分由通过切分点并与坐标轴x(1)垂直的超平面实现.')
        print('  由根结点生成深度为1的左、右子结点对应坐标x(1)小于切分点的子区域,右子结点对应于坐标x(1)大于切分点的子区域')
        print('  (2) 重复:对深度为j的结点,选择x(l)为切分的坐标轴,l=j(mod k) + 1,',
            '以该结点的区域中所有实例的x(l)坐标的中位数为切分点,将该结点对应的超矩形区域切分为两个子区域.',
            '切分由通过切分点并与坐标轴x(l)垂直的超平面实现')
        print('  由该结点生成深度')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')

chapter3 = Chapter3()

def main():
    chapter3.note()

if __name__ == '__main__':
    main()