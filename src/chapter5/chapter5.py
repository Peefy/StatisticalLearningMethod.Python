
class Chapter5:
    """
    第5章 决策树
    """
    def __init__(self):
        """
        第5章 决策树模型与学习
        """
        pass

    def note(self):
        """
        chapter5 note
        """
        print('第5章 决策树模型')
        print('5.1 决策树模型与学习')
        print('5.1.1 决策树模型')
        print('决策树是一种基本的分类与回归方法.本章主要讨论用于分类的决策树.',
            '决策树模型呈树形结构.在分类问题中,表示基于特征对实例进行分类的过程',
            '可以认为是if—then规则的集合,也可以认为是定义在特征空间与类空间上的条件概率分布.',
            '其主要优点是模型具有可读性,分类速度快.学习时,利用训练数据,根据损失函数最小化的原则建立决策树模型.',
            '预测时,对新的数据,利用决策树模型进行分类.')
        print('决策树学习通常包括3个步骤:特征选择、决策树的生成和决策树的修剪.')
        print('5.1 决策树模型与学习')
        print('5.1.1 决策树模型')
        print('定义5.1(决策树)分类决策树模型是一种描述对实例进行分类的树形结构.',
            '决策树由结点(node)和有向边(directed edge)组成.结点有两种类型：',
            '内部结点(internal node)和叶结点(leaf mode).内部结点表示一个特征或属性,',
            '叶结点表示一个类')
        print('用决策树分类,从根结点开始,对实例的某一特征进行测试,根据测试结果,',
            '将实例分配到其子结点;这时,每一个子结点对应着该特征的一个取值.',
            '如此递归地对实例进行测试并分配,直至达到叶结点.最后将实例分到叶结点的类中')
        print('5.1.2 决策树与if-then规则')
        # !可以将决策树看成一个if-then规则的集合
        print('可以将决策树看成一个if-then规则的集合.将决策树转换成if-then规则过程是这样的:',
            '由决策树的根结点到叶结点的每一条路径构建一条规则;',
            '路径上的内部结点的特征对应着规则的条件,而叶结点的类对应着规则的结论.',
            '决策树的路径或其对应的if-then规则集合具有一个重要的性质:互斥并且完备,',
            '这就是说,每一个实例都被一条路径或一条规则所覆盖,而且只被一条路径或一条规则所覆盖.',
            '这里所谓覆盖是指实例的特征与路径上的特征一致或实例满足规则的条件')
        print('5.1.3 决策树与条件概率分布')
        print('决策树还表示给定特征条件下类的条件概率分布.这一条件概率分布定义在特征空间的一个划分(partition)上.',
            '将特征空间划分为互不相交的单元(cell)或区域(region),并在每个单元定义一个类的概率分布就构成了一个条件概率分布',
            '由各个单元给定条件下类的条件概率分布组成.','假设X为表示特征的随机变量,Y为表示类的随机变量,',
            '那么这个条件概率分布可以表示为P(Y|X).X取值于给定划分下单元的集合,Y取值于类的集合.',
            '各叶结点(单元)上的条件概率往往偏向某一个类,即属于某一类的概率较大.',
            '决策树分类时将该结点的实例强行分到条件概率大的那一类去.')
        print('5.1.4 决策树学习')
        print('假设给定训练数据集D={(x1,y1),(x2,y2),...,(xn,yn)}')
        print('其中,xi=(xi(1),xi(2),...,xi(n))^T为输入实例(特征向量),n为特征个数,yi∈{1,2,...,K}为类标记,',
            'i=1,2,...,N,N为样本容量.学习的目标是根据给定的训练数据集构建一个决策树模型,使它能够对实例进行正确的分类.')
        print('决策树学习本质上是从训练数据集中归纳出一组分类规则.与训练数据集不相矛盾的决策树',
            '(即能对训练数据进行正确分类的决策树)可能有多个,也可能一个也没有')
        print('需要的是一个与训练数据矛盾较小的决策树,同时具有很好的泛化能力.从另一个角度看,',
            '决策树学习是由训练数据集估计条件概率模型.基于特征空间划分的类的条件概率模型有无穷多个.',
            '选择调剂那概率模型应该不仅对训练数据由很好的拟合,而且对未知数据有很好的预测.')
        print('决策树学习用损失函数表示这一目标.如下所述,决策树学习的损失函数通常是正则化的极大似然函数.',
            '决策树学习的策略是以损失函数为目标函数的最小化.')
        print('当损失函数确定以后,学习问题就变为在损失函数意义下的选择最优决策树的问题.',
            '因为从所有可能的决策树中选取最优决策树是NP完全问题,所以现实中决策树学习算法通常采用启发式方法,',
            '近似求解这一最优化问题.这样的得到的决策树是次最优的.')
        print('决策树学习的算法通常是一个递归地选择最优特征,并根据该特征对训练数据进行分割,',
            '使得对各个子数据集有一个最好的分类的过程.这一过程对应着特征空间的划分,对应着决策树的构建.')
        print('开始,构建根结点,将所有的训练数据都放在根结点.选择一个最优特征,按照这一特征将训练数据集分割成子集,',
            '使得各个子集有一个当前条件下最好的分类.如果这些子集已经能够被基本正确分类,',
            '那么构建叶结点,并将这些子集分到所对应的叶结点中去;如果还有子集不能基本正确分类,',
            '那么就对这些子集选择新的最优特征,继续对其进行分割,构建相应的结点.',
            '如此递归地进行下去,直至所有的训练数据子集被基本正确分类,或者没有合适的特征为止.',
            '最后每个子集都被分到叶结点上,即都有了明确的类.这就生成了一棵决策树')
        # !剪枝可以提高决策树预测的能力，避免过拟合
        print('以上方法生成的决策树可能对训练数据有很好的分类能力,但对未知的测试数据却未必有很好的分类能力,',
            '即可能发生过拟合现象.需要对已经生成的树自上而下进行进行剪枝,将树变得简单,从而使它具有更好的泛化能力',
            '具体地,就是去掉过于细分的叶结点,使其回退到父结点,甚至更高的结点,然后将父结点或更高的结点改为新的叶结点')
        print('如果特征数量很多,也可以在决策树学习开始的时候,对特征进行选择,只留下对训练数据有足够分类能力的特征.',
            '可以看出,决策树学习算法包含特征选择、决策树的生成与决策树的剪枝过程.',
            '由于决策树表示一个条件概率分布,所以深浅不同的决策树对应着不同复杂度的概率模型.',
            '决策树的生成对应于模型的局部选择,决策树的剪枝对应于模型的全局选择.',
            '决策树的生成只考虑局部最优,相对地,决策树的剪枝则考虑全局最优')
        print('决策树学习常用的算法有ID3、C4.5与CART,下面结合这些算法分别叙述决策树学习的特征选择、',
            '决策树的生成和剪枝过程')
        print('5.2 特征选择')
        print('5.2.1 特征选择问题')
        print('特征选择在于选取对训练数据具有分类能力的特征.这样可以提高决策树学习的效率.',
            '如果利用一个特征进行分类的结果与随机分类的结果没有很大差别,则称这个特征是没有分类能力的',
            '经验上扔掉这样的特征对决策树学习的精度影响不大.通常特征选择的准则是信息递增或信息增益比.')
        print('例5.1 表5.1是一个由15个样本组成的贷款申请训练数据.数据包包括贷款申请人的4个特征(属性):',
            '第1个特征是年龄,有3个可能值:青年,中年,老年;第2个特征是有工作,有2个可能值:是,否;',
            '第3个特征是有自己的房子,有2个可能值:是,否;',
            '第4个特征是信贷情况,有3个可能值:非常好,好,一般.',
            '表的最后一列是类别,是否同意贷款,取2个值:是,否.')
        print('希望通过所给的训练数据学习一个贷款申请的决策树,用以对未来的贷款申请进行分类,',
            '即当新的客户提出贷款申请时,根据申请人的特征利用决策树决定是否批准贷款申请.')
        # !特征选择是决定用哪个特征来划分特征空间.
        print('特征选择是决定用哪个特征来划分特征空间.')
        print('5.2.2 信息增益')
        print('在信息论与概率统计中,熵是表示随机变量不确定性的度量,',
            '设X是一个取有限个值的离散随机变量,其概率分布为:P(X=xi)=pi,i=1,2,..,n',
            '则随机变量X的熵定义为:H(X)=-∑pilogpi;若pi=0,则定义0log0=0,对数以2为底或以e为底(自然对数)',
            '这时熵的单位分别称作比特(bit)或纳特(nat).由定义可知,熵只依赖于X的分布,而与X的取值无关,',
            '所以也可将X的熵记做H(p),即H(p)=-∑pilogpi')
        print('熵越大,随机变量的不确定性就越大.从定义可验证:0<H(p)<=logn')
        print('当随机变量只取两个值,例如1,0时,即X的分布为:P(X=1)=p,P(X=0)=1-p,0<=p<=1',
            '熵为H(p)=-plog2p-(1-p)log2(1-p)')
        print('这时,熵H(p)随概率p变化的曲线如图5.4所示(单位为比特)')
        print('当p=0或p=1时H(p)=0,随机变量完全没有不确定性.当p=0.5时,H(p)=1,熵取值最大,随机变量不确定性最大.')
        print('设有随机变量(X,Y),其联合概率分布为:P(X=xi,Y=yi)=pij,i=1,2,...,n;j=1,2,...,m')
        print('条件熵H(Y|X)表示在已知随机变量X的条件下随机变量Y的不确定性.随机变量X给定的条件下随机变量Y的条件熵H(Y|X)',
            '定义为X给定条件下Y的条件概率分布的熵对X的数学期望H(Y|X)=∑piH(Y|X=xi)',
            '这里,pi=P(X=xi),i=1,2,...,n')
        print('当熵和条件熵中的概率由数据估计(特别是极大似然估计)得到时,所对应的熵与条件熵分别称为经验熵和经验条件熵.',
            '此时，如果有0概率,令0log0=0')
        print('信息增益表示得知特征X的信息而使得类Y的信息不确定性减少的程度')
        print('定义5.2(信息增益)特征A对训练数据集D的信息增益g(D,A),',
            '定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵H(D|A)之差,即',
            'g(D,A)=H(D)-H(D|A)')
        print('一般地,熵H(Y)与条件熵H(Y|X)之差称为互信息.决策树学习中的信息增益等价于训练数据集中类与特征的互信息')
        print('决策树学习应用信息增益准则选择特征.给定训练数据集D和特征A,经验熵H(D)表示对数据集D进行分类的不确定性.',
            '而经验条件熵H(D|A)表示在特征值A给定的条件下对数据集D进行分类的不确定性.',
            '那么它们的差,即信息增益,就表示由于特征A而使得对数据集D的分类的不确定性减少的程度.',
            '显然,对于数据集D而言,信息增益依赖于特征,不同的特征往往具有不同的信息增益.',
            '信息增益大的特征具有更强的分类能力')
        print('根据信息增益准则特征选择方法是:对训练数据集(或子集)D,计算其每个特征的信息增益,',
            '并比较他们的大小,选择信息增益最大的特征.')
        print('')
        print('')
        print('')
        print('')
        print('')

chapter5 = Chapter5()

def main():
    chapter5.note()

if __name__ == '__main__':
    main()