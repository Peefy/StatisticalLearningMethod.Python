# 第7章 支持向量机

支持向量机(support vector machines, SVM)是一种二类分类器.它的基本模型是定义在特征空间上的间隔最大的线性分类器,',
            '间隔最大使它有别于感知机；支持向量机还包括核技巧,这使它成为实质上的非线性分类器.',
            '支持向量机的学习策略就是间隔最大化,可形式化为一个求解凸二次规划(convex quadratic programming)的问题,',
            '也等价于正则化的合页损失函数的最小化问题.支持向量机的学习算法是求解凸二次规划的最优化算法.
            
1.支持向量机最简单的情况是线性可分支持向量机,或硬间隔支持向量机. 构建它的条件是训练数据线性可分.其学习策略是最大间隔法.可以表示为凸二次规划问题,其原始最优化问题为:
 min 0.5||w||^2 s.t. yi(w·xi+b)-1>=0,i=1,2,...,N
求得最优化问题的解为w*,b*,得到线性可分支持向量机,分离超平面是w*·x+b*=0
分类决策函数是f(x)=sign(w*·x+b*)
最大间隔法中,函数间隔与几何间隔是重要的概念.
线性可分支持向量机的最优解存在且唯一.
通常,通过求解对偶问题学习线性可分支持向量机,即首先求解对偶问题的最优值a*,然后求最优值w*和b*, 得出分离超平面和分类决策函数.
2.现实中训练数据是线性可分的情形较少训练数据往往是近似线性可分的,这时使用线性支持向量机, 或软间隔支持向量机.线性支持向量机是最基本的支持向量机.
对于噪声或例外,通过引入松弛变量fi,使其“可分”,得到线性支持向量机学习的凸二次规划问题, 其原始最优化问题是.求解原始最优化问题的解w*,b*,得到线性支持向量机,其分离超平面为:
  w*·x+b*=0
分类决策函数为f(x)=sign(w*·x+b*)
线性可分支持向量机的解w*唯一但b*不唯一.
线性支持向量机学习等价于最小化二阶范数正则化的合页函数.
3.非线性支持向量机
对于输入空间中的非线性分类问题,可以通过非线性变换将它转化为某个高维特征空间中的线性分类问题, 在高维特征空间中学习线性支持向量机.
在线性支持向量机学习的对偶问题中,用核函数K(x,z)代替内积,求解得到的就是非线性支持向量机
4.SMO算法
  SMO算法是支持向量机学习的一种快速算法,其特点是不断将原二次规划问题分解为只有两个变量的二次规划子问题, 并对子问题进行解析求解,直到所有变量满足KKT条件为止. 这样通过启发式的方法得到原二次规划问题的最优解.因为子问题有解析解, 所以每次计算子问题都很快,虽然计算子问题次数很多,但在总体上还是高效的.
