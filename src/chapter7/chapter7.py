
class Chapter7:
    """
    第7章 支持向量机
    """
    def __init__(self):
        """
        第7章 支持向量机
        """
        pass

    def note(self):
        """
        chapter7 note
        """
        print('第7章 支持向量机')
        print('支持向量机(support vector machines, SVM)是一种二类分类器.它的基本模型是定义在特征空间上的间隔最大的线性分类器,',
            '间隔最大使它有别于感知机；支持向量机还包括核技巧,这使它成为实质上的非线性分类器.',
            '支持向量机的学习策略就是间隔最大化,可形式化为一个求解凸二次规划(convex quadratic programming)的问题,',
            '也等价于正则化的合页损失函数的最小化问题.支持向量机的学习算法是求解凸二次规划的最优化算法.')
        print('支持向量机学习方法包含构建由简至繁的模型:线性可分支持向量机、线性支持向量机及非线性支持向量机.',
            '简单模型是复杂模型的基础,也是复杂模型的特殊情况.当训练数据线性可分时,通过硬间隔最大化,',
            '也学习一个线性的分类器,即线性支持向量机,又称为软间隔支持向量机;',
            '当训练数据线性不可分时,通过使用核技巧及软间隔最大化,学习非线性支持向量机.')
        print('当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时,',
            '核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积',
            '通过使用核函数可以学习非线性支持向量机,等价于隐式地在高维的特征空间中学习线性支持向量机.',
            '核方法是比支持向量机更一般的机器学习方法')
        print('7.1 线性可分支持向量机与硬间隔最大化.')
        print('7.1.1 线性可分支持向量机')
        print('考虑一个二类分类问题.假设输入空间与特征空间为两个不同的空间.',
            '输入空间为欧式空间或离散集合,特征空间为欧式空间或希尔伯特空间.',
            '线性可分支持向量机、线性支持向量机假设这两个空间的元素一一对应,并将输入空间中的输入映射为特征空间中的特征向量.',
            '非线性支持向量机利用一个从输入空间到特征空间的非线性映射将输入映射为特征向量.',
            '输入都由输入空间转换到特征空间,支持向量机的学习是在特征空间进行的.')
        print('假设给定一个特征空间上的训练数据集T={(x1,y1),(x2,y2),...,(xn,yn)}')
        print('其中,xi∈X=R^n,yi∈Y={+1,-1},i=1,2,...,N,xi为第i个特征向量,也称为实例,',
            'yi为xi的类标记,当yi=+1时,称xi为正例;当yi=-1时,称xi为负例,',
            '(xi,yi)称样本点.再假设训练数据集是线性可分的.')
        # !SVM学习的目标是在特征空间中找到一个分离超平面,能将实例分到不同的类.
        print('SVM学习的目标是在特征空间中找到一个分离超平面,能将实例分到不同的类.')
        print('分离超平面对应于方程w·x+b=0,由法向量w和截距b决定,可用(w,b)来表示.')
        print('分离超平面将特征空间划分为两部分,一部分是正类,一部分是负类.',
            '法向量指向一侧为正类,另一侧为负类.')
        print('一般地,当训练数据集线性可分时,存在无穷个分离超平面可将两类数据正确分开.',
            '感知机利用误分类最小的策略,求得分离超平面,不过这时的解有无穷多个.',
            '线性可分支持向量机利用间隔最大化求最优分离超平面,这时,解是唯一的')
        print('定义7.1 (线性可分支持向量机)给定线性可分训练数据集,通过间隔最大化或等价地求解相应的',
            '凸二次规划问题学习得到的分离超平面为：w*·x+b*=0 以及相应的决策函数:f(x)=sign(w*·x+b*)',
            '称为线性可分支持向量机')
        print('7.1.2 函数间隔和几何间隔')
        print('在图7.1中,有A,B,C三个点,表示3个实例,均在分离超平面的正类一侧,预测它们的类.')
        print('一般来说,一个点距离分离超平面的远近可以表示分类预测的确信程度.在超平面wx+b=0确定的情况下',
            '|wx+b|能够相对地表示点x距离超平面的远近.而wx+b的符号与类标记y的符号是否一致能够表示分类是否正确.',
            '所以可用量y(wx+b)来表示分类的正确性及确信度,这就是函数间隔的概念.')
        print('定义7.2（函数间隔）对于给定的训练数据集T和超平面(w,b),定义超平面(w,b)关于样本点(xi,yi)的函数间隔为:',
            'yi=yi(w·wi+b).定义超平面(w,b)关于训练数据集T的函数间隔为超平面(w,b)关于T中所有样本点(xi,yi)的函数间隔之最小值,即',
            'y\'=minyi')
        print('函数间隔可以表示分类预测的正确性及确信度.但是选择分离超平面时,只有函数间隔还不够.',
            '因为只要成比例地改变w和b，例如将它们改为2w和2b,超平面并没有改变,',
            '但函数间隔却成为原来的2倍.可以对分离超平面的法向量w加某些约束,如规范化,||w||=1,使得间隔是确定的.',
            '这时函数间隔成为几何间隔(geometric margin)')
        print('图7.2给出了超平面(w,b)及其法向量w.点A表示某一实例xi,其类标记为yi=+1.',
            '点A与超平面(w,b)的距离由线段AB给出,记作yi=w/||w||·xi+b/||w||')
        print('其中,||w||为w的L2范数.这是点A在超平面正的一侧的情形.如果点A在超平面负的一侧,即yi=-1,',
            '那么点与超平面的距离yi=-(w/||w||·xi+b/||w||)')
        print('一般地,当样本点(xi,yi)被超平面(w,b)正确分类时,点xi与超平面(w,b)的距离是',
            'di=yi(w/||w||·xi+b/||w||)')
        print('定义7.3(几何间隔)对于给定的训练数据集T和超平面(w,b),定义超平面(w,b)关于样本点(xi,yi)的几何间隔为,',
            'di=yi(w/||w||·xi+b/||w||)')
        print('定义超平面(w,b)关于训练数据集T的几何间隔为超平面(w,b)',
            '关于T中所有样本点(xi,yi)的几何间隔之最小值,即d=mindi')
        print('超平面(w,b)关于样本点(xi,yi)的几何间隔一般是实例点到超平面的带符号的距离,',
            '当样本点被超平面正确分类时就是实例点到超平面的距离.')
        print('从函数间隔和几何间隔的定义可知,函数间隔和几何间隔有下面的关系:di=di/||w||;d=d/||w||')
        print('如果||w||=1,那么函数间隔和几何间隔相等.如果超平面参数w和b成比例地改变(超平面没有改变),',
            '函数间隔也按此比例改变,而几何间隔不变.')
        print('7.1.3 间隔最大化')
        print('SVM学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面.',
            '对线性可分的训练数据集而言,线性可分分离超平面有无穷多个(等价于感知机),但是几何间隔最大的分离超平面是唯一的.',
            '这里的间隔最大化又称为硬间隔最大化.')
        print('1.最大间隔分离超平面')
        print('可以表述为一个约束最优化问题 maxd s.t. yi(w/||w||·xi+b/||w||)>=d, i=1,2,...,N')
        print('注意到最大化1/||w||和最小化0.5||w||^2是等价的,于是就得到下面的线性可分支持向量机学习的最优化问题:')
        print('min 0.5||w||^2. s.t. yi(wxi+b)-1>=0')
        print('这是一个凸二次规划(convex quadratic programming)问题.')
        print('凸优化问题是指约束最优化问题')
        print('min f(w)  s.t.gi(w)<=0 i=1,2,...,k, hi(w)=0, i=1,2,...,l')
        print('其中,目标函数f(w)和约束函数gi(w)都是R^n上的连续可微的凸函数,约束函数hi(w)是R^n上的仿射函数.')
        print('当目标函数f(w)是二次函数且约束函数gi(w)是仿射函数时,上述凸最优化问题称为凸二次规划问题.')
        print('如果求出了约束最优化问题的解w*,b*,那么就可以得到最大间隔分离超平面w*·x+b*=0及分类决策函数',
            'f(x)=sign(w*·x+b*),即线性可分支持向量机模型.')
        print('算法7.1(线性可分支持向量机学习算法----最大间隔法)')
        print('输入:线性可分训练数据集T={(x1,y1),(x2,y2),...,(xn,yn)},其中,xi∈X=R^n,yi∈Y={-1,+1},i=1,2,...,N;')
        print('输出:最大间隔分离超平面和分类决策函数')
        print('(1) 构造并求解约束最优化问题:min 0.5||w||^2 s.t. yi(w·xi+b)-1>=0, i=1,2,...,N',
            '求得最优解w*,b*')
        print('(2) 由此得到分离超平面w*·x+b*=0,分类决策函数f(x)=sign(w*·x+b*)')
        print('2.最大间隔分离超平面的存在唯一性.')
        print('线性可分训练数据集的最大间隔分离超平面是存在且唯一的.')
        print('定理7.1 (最大分隔分离超平面的存在唯一性) 若训练数据集T线性可分,则可将训练数据集中的样本点',
            '完全正确分开的最大间隔分离超平面存在且唯一.')
        print('3.支持向量和间隔边界')
        print('在线性可分情况下,训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量.',
            '支持向量是使约束条件等号成立的点,即:yi(w·xi+b)-1=0')
        print('对yi=+1的正例点,支持向量在超平面H1:w·x+b=1上')
        print('对yi=-1的负例点,支持向量在超平面H2:w·x+b=-1上')
        print('在H1和H2上的点就是支持向量')
        print('注意到H1和H2平行,并且米有实例点落在它们中间.在H1与H2之间形成一条长带,分离超平面与它们平行且位于它们中央.',
            '长带的宽度,即H1与H2之间的距离称为间隔.间隔依赖于分离超平面的法向量w,等于2/||w||.',
            'H1和H2称为间隔边界')
        print('在决定分离超平面只有支持向量起作用,而其他实例点并不起作用.如果移动支持向量将改变所求的解;',
            '但是如果在间隔边界以外移动其他实例点,甚至去掉这些点,则解不会改变.')
        print('由于支持向量在确定分离超平面中起着决定性作用,所以将这种分类模型称为支持向量机.',
            '支持向量的个数一般很少,所以支持向量机由很少的“重要的”训练样本确定.')
        print('例7.1 数据与例2.1相同.已知一个训练集,其实例点是x1=(3,3)^T,x2=(4,3)^T,负实例点是x3=(1,1)^T,',
            '试求最大间隔分离超平面.')
        print('解,按照算法7.1,根据训练数据集构造约束最优化问题:')
        print('min 0.5(w1^2+w2^2) s.t. 3w1+3w2+b>=1 4w1+3w2+b>=1 -w1-w2-b>=1')
        print('求得此最优化问题的解w1=w2=0.5, b=-2. 于是最大间隔分离超平面为0.5x(1)+0.5x(2)-2=0',
            '其中,x1=(3,3)^T与x3=(1,1)^T为支持向量.')
        print('7.1.4 学习的对偶算法')
        print('为了求解线性可分支持向量机的最优化问题,将它作为原始最优化问题,应用拉格朗日对偶性,',
            '通过求解对偶问题得到原始问题的最优解,这就是线性可分支持向量机的对偶算法.')
        print('优点：对偶问题往往更容易求解；自然引入核函数,进而推广到非线性分类问题.')
        print('首先构建拉格朗日函数.为此,对每一个不等式约束引进拉格朗日乘子ai>=0,i=1,2,...,N,定义拉格朗日函数：')
        print('L(w,b,a)=0.5||w||^2-∑aiyi(w·xi+b)+∑ai')
        print('其中,a=(a1,a2,...,aN)^T为拉格朗日乘子向量.')
        print('根据拉格朗日对偶性,原始问题的对偶问题是极大极小问题：maxminL(w,b,a)')
        print('(1) minL(w,b,a)')
        print('  将拉格朗日函数L(w,b,a)分别对w,b求偏导并令其等于0.')
        print('(2) 求minL(w,b,a)对a的极大,即是对偶问题')
        print('  并且将目标函数由求极大转换为求极小,就得到等价的对偶最优化问题')
        print('综上所述,对于给定的线性可分训练数据集,可以首先求对偶问题的解a*,',
            '再利用公式求得原始问题的解w*,b*;从而得到分离超平面及分类决策函数.',
            '这种算法称为线性可分支持向量机的对偶学习算法,',
            '是线性可分支持向量机学习的基本算法.')
        print('算法7.2 (线性可分支持向量机学习算法)')
        print('输入:线性可分训练集T={(x1,y1),(x2,y2),...,(xn,yn)},其中xi∈X=R^n,',
            'yi∈Y={-1,+1},i=1,2,...,N;')
        print('输出:分离超平面和分类决策函数.')
        print('(1) 构造并求解约束最优化问题')
        print('  min 0.5∑∑aiajyiyj(xi·xj)-∑ai s.t. ∑aiyi=0 ai>=0,i=1,2,...,N,',
            '求得最优解a*=(a1*,a2*,...,aN*)^T')
        print('(2) 计算 w*=∑ai*yixi 并选择a*的一个正分量aj*>0,计算 b*=yj-∑aiyi(xi·xj)')
        print('(3) 求得分离超平面 w*·x+b*=0')
        print('分类决策函数:f(x)=sign(w*·x+b*)')
        print('在线性可分支持向量机中,w*和b*只依赖于训练数据中对应于ai*>0的样本点(xi,yi),',
            '而其他样本点对w*和b*没有影响.将训练数据中对应于ai*>0的实例点xi∈R^n称为支持向量.')
        print('在线性可分支持向量机中,w*和b*只依赖于训练数据中对应于ai>0的样本点对w*和b*没有影响.',
            '将训练数据中对应于ai>0的实例点xi∈R^n称为支持向量.')
        print('定义7.4(支持向量)考虑原始最优化问题及对偶优化问题,',
            '将训练数据集中对应于ai*>0的样本点(xi,yi)的实例xi∈R^n称为支持向量.')
        print('根据这一定义,支持向量一定在间隔边界上.由KKT互补条件可知,',
            'ai*(yi(w*·xi+b*)-1)=0,i=1,2,...,N')
        print('对应于ai*>0的实例xi,有yi(w*·xi+b*)-1=0或w*·xi+b*=+-1')
        print('即xi一定在间隔边界上.这里的支持向量的定义与前面给出的支持向量的定义是一致的.')
        print('例7.2 训练数据与例7.1相同.正例点是x1=(3,3)^T,x2=(4,3)^T,',
            '负例点是x3=(1,1)^T')
        print('解 根据所给数据,对偶问题是:')
        print('min 0.5∑∑aiajyiyj(xi·xj)-∑ai=',
            '0.5(18a1^2+25a2^2+2a3^2+42a1a2-12a1a3-14a2a3)-a1-a2-a3')
        print('s.t. a1+a2-a3=0  a>=0, i=1,2,3 ')
        print('解这一最优化问题.将a3=a2+a1代入目标函数并记为s(a1,a2)=4a1^2+13/2a2^2+10a1a2-2a1-2a2')
        print('对a1,a2求偏导数并令其为0,易知s(a1,a2)在点(1.5,-1)^T取极值,但该点不满足约束条件a2>=0,',
            '所以最小值应在边界上达到.')
        print('当a1=0时,最小值s(0,2/13)=-2/13;当a2=0时,最小值s(0.25,0)=-0.25.',
            '于是s(a1,a2)在a1=0.25,a2=0达到最小,此时a3=a1+a2=0.25')
        print('这样,a1*=a2*=0.25对应的实例点x1,x3是支持向量.w1*=w2*=0.5, b*=-2')
        print('分离超平面为:0.5x(1)+0.5x(2)-2=0')
        print('分离决策函数为:f(x)=sign(0.5x(1)+0.5x(2)-2)')
        print('对于线性可分问题,上述线性可分支持向量机的学习(硬间隔最大化)算法是完美的.',
            '但是,训练数据集线性可分是理想的情形.在现实问题中,训练数据集往往是线性不可分的,',
            '即在样本中出现噪声或特异点.此时,有更一般的学习算法.')
        print('7.2 线性支持向量机与间隔最大化.')
        print('7.2.1 线性支持向量机')
        print('线性可分问题的支持向量机学习方法,对线性不可分训练数据是不适用的,',
            '因为这时上述方法的不等式约束并不能都成立.就需要修改硬间隔最大化,使其成为软间隔最大化.')
        print('假设给定一个特征空间上的训练数据集T={(x1,y1),(x2,y2),...,(xn,yn)}')
        print('其中,xi∈X=R^n,yi∈Y={+1,-1},i=1,2,...,N,xi为第i个特征向量,yi为xi的类标记.',
            '再假设训练数据集不是线性可分的.通常情况是,训练数据中有一些特异点(outlier),',
            '将这些特异点除去后,剩下大部分的样本点组成的集合是线性可分的')
        print('线性不可分意味着某些样本点(xi,yi)不能满足函数间隔大于等于1的约束条件.',
            '为了解决这个问题,可以对每个样本点(xi,yi)引进一个松弛变量fi>=0',
            '使函数间隔加上松弛变量大于等于1.这样,约束条件变为:yi(w·xi+b)>=1-fi')
        print('同时,对每个松弛变量fi,支付一个代价fi.目标函数由原来的0.5||w||^2变成0.5||w||^2+C∑fi')
        print('这里,C>0称为惩罚参数,一般由应用问题决定,C值大时对误分类的惩罚增大,',
            'C值小时对误分类的惩罚减小.最小化目标函数包含两层含义：使0.5||w||^2尽量小即间隔尽量大,',
            '同时使误分类点的个数尽量小,C是调和二者的系数.')
        print('有了上面的思路,可以和训练数据集线性可分时一样来考虑训练数据集线性不可分时的线性支持向量机学习问题.',
            '相应于硬间隔最大化,称为软间隔最大化.')
        print('线性不可分的线性支持向量机的学习问题变成如下凸二次规划问题(原始问题):')
        print('min 0.5||w||^2+C∑fi  s.t. yi(w·xi+b)>=1-fi,i=1,2,...,N fi>=0,i=1,2,...,N')
        print('原始问题是一个凸二次规划问题,因而关于(w,b,f)的解是存在的.可以证明w的解是唯一的,但b的解不唯一,',
            'b的解存在于一个区间.')
        print('设原始问题的解是w*,b*,于是可以得到分离超平面w*·x+b*=0及分类决策函数',
            'f(x)=sign(w*·x+b*).称这样的模型为训练样本线性不可分时的线性支持向量机,',
            '简称为线性支持向量机.显然,线性支持向量机包含线性可分支持向量机.',
            '由于现实中训练数据往往是线性不可分的,线性支持向量机具有更广的适用性.')
        print('定义7.5(线性支持向量机) 对于给定的线性不可分的训练数据集,通过求解凸二次规划问题,',
            '即软间隔最大化问题,得到的分离超平面为:w*·x+b*=0以及相应的分类决策函数f(x)=sign(w*·x+b*)')
        print('称为线性支持向量机')
        print('7.2.2 学习的对偶算法')
        print('原始问题的对偶问题是:')
        print('min 0.5∑∑aiajyiyj(xi·xj)-∑ai s.t. ∑aiyi=0 0<=ai<=C,i=1,2,...,N')
        print('原始最优化问题的拉格朗日函数是:')
        print('L(w,b,f,a,u)=0.5||w||^2+C∑fi-∑ai(yi(w·xi+b)-1+fi)-∑uifi, 其中ai>=0,ui>=0')
        print('对偶问题是拉格朗日函数的极大极小问题.首先求L(w,b,f,a,u)对w,b,f的极小,由')
        print('grad(L(w,b,f,a,u))=w-∑aiyixi=0')
        print('grad(L(w,b,f,a,u))=-∑aiyi=0')
        print('grad(L(w,b,f,a,u))=C-ai-ui=0')
        print('得')
        print('w=∑aiyixi;  ∑aiyi=0;  C-ai-ui=0')
        print('得 minL(w,b,f,a,u)=-0.5∑∑aiajyiyj(xi·xj)+∑ai')
        print('再对minL(w,b,f,a,u)求a的极大,即得对偶问题：')
        print('max -0.5∑∑aiajyiyj(xj·xi)+∑ai')
        print('s.t. ∑aiyi=0;  C-ai-ui=0  ai>=0  ui>=0,i=1,2,...,N')
        print('可以通过求解对偶问题而得到原始的解,进而确定分离超平面和决策函数.为此,',
            '就可以定理的形式叙述原始问题的最优解和对偶问题的最优解的关系.')
        print('定理7.3 设a*=(a1*,a2*,...,aN*)^T是对偶问题的一个解,若存在a*的一个分量aj*,0<aj*<C,',
            '则原始问题的解w*,b*可按下式求得;')
        print('  w*=∑ai*yixi;   b*=yj-∑yiai*(xi·xj)')
        print('算法7.3 (线性支持向量机学习算法)')
        print('输入:训练数据集T={(x1,y1),(x2,y2),...,(xn,yn)},其中,xi∈X=R^n,yi∈Y={-1,+1},i=1,2,..,N',
            '输出:分离超平面和分类决策函数.')
        print('(1) 选择惩罚函数C>0,构造并求解凸二次规划问题')
        print('  min 0.5∑∑aiajyiyj(xi·xj)-∑ai s.t. ∑aiyi=0 0<=ai<=C, i=1,2,...,N')
        print('  求得最优解a*=(a1*,a2*,...,aN*)^T')
        print('(2) 计算w*=∑ai*yixi,选择a*的一个分量aj*适合条件0<aj*<C,计算b*=yi-∑yiai*(xi·xj)')
        print('(3) 求得分离超平面w*·x+b*=0')
        print('  分类决策函数:f(x)=sign(w*·x+b*)')
        print('步骤(2)中,对任一适合条件0<aj*<C的aj*,按上式都可求出b*,但是由于原始问题对b的解并不唯一,',
            '所以实际计算时可以取在所有符合条件的样本点上的平均值')
        print('7.2.3 支持向量')
        print('在线性不可分的情况下,将对偶问题的解a*=(a1*,a2*,...,aN*)^T中对应于ai*>0的样本点(xi,yi)',
            '的实例xi称为支持向量(软间隔的支持向量).这时的支持向量要比线性可分时的情况复杂一些.',
            '图中,分离超平面由实线表示,间隔边界由虚线表示,正实例点由“。”表示,负实例点由“×”表示.',
            '图中还标出了实例xi到间隔边界的距离fi/||w||.')
        print('软间隔的支持向量xi或者在间隔边界上,或者在间隔边界与分离超平面之间,',
            '或者在分离超平面误分一侧.若ai*<C,则fi=0,支持向量xi恰好落在间隔边界上;',
            '若ai*=C,0<fi<1,则分类正确,xi在间隔边界与分离超平面之间;',
            '若ai*=C,fi=1,则xi在分离超平面;若ai*=C,fi>1,则xi位于分离超平面误分一侧.')
        print('7.2.4 合页损失函数.')
        print('对于线性支持向量机学习来说,其模型为分离超平面w*·x+b*=0及决策函数f(x)=sign(w*·x+b*),',
            '其学习策略为软间隔最大化,学习算法为凸二次规划.')
        print('线性支持向量机学习还有另外一种解释,就是最小化以下目标函数:',
            '∑[1-yi(w·xi+b)]+la||w||^2')
        print('目标函数的第1项是经验损失或经验风险,函数')
        print('L(y(w·x+b))=[1-y(w·x+b)]+称为合页损失函数(hinge loss function)',
            '下标+表示一下取正值的函数:[z]+ =z,z>0; [z]+ =0,z<=0')
        print('这就是说,当样本点(xi,yi)被正确分类且函数间隔(确信度) yi(w·xi+b)大于1时,损失时0,',
            '否则损失是1-yi(w·xi+b),注意在图7.5中的实例点x4被正确分类,但损失不是0.',
            '目标函数第2项是系数为d的w的L2范数,是正则化项.')
        print('定理7.4 线性支持向量机原始问题最优化问题:')
        print('min 0.5||w||^2+C∑fi. s.t. yi(w·xi+b)>=1-fi, i=1,2,...,N fi>=0,i=1,2,...,N. fi>=0,i=1,2,...,N')
        print('等价于最优化问题 min ∑[1-yi(w·xi+b)]+ +d||w||^2')
        print('可将最优化问题写成问题,令1-yi(w·xi+b)=fi,fi>=0.')
        print('则yi(w·xi+b)>=1,于是w,b,fi满足约束条件.有：',
            '[1-yi(w·xi+b)]+=[fi]+=fi,所以最优化问题可以写成:',
            'min ∑fi+d||w||^2,若取d=1/2C,则',
            'min 1/C(1/2||w||^2+C∑fi)')
        print('7.3 非线性支持向量机与核函数')
        print('对解线性分类问题,线性分类支持向量机是一种非常有效的方法.但是,',
            '优势分类问题是非线性的,这时可以使用非线性支持向量机.',
            '本节叙述非线性支持向量机,其主要特点是利用核技巧.',
            '核技巧不仅应用于支持向量机,而且应用于其他统计学习问题.')
        print('7.3.1 核技巧')
        print('1.非线性分类问题')
        print('非线性分类问题是指通过利用非线性模型才能很好地进行分类的问题.')
        print('一般来说,对给定的一个训练数据集T={(x1,y1),(x2,y2),...,(xn,yn)},其中,',
            '实例xi属于输入空间,xi∈X=R^n,对应的标记有两类yi∈Y={-1,+1},i=1,2,...,N',
            '如果能用R^n中的一个超曲面将正负例正确分开,则称这个问题为非线性可分问题.')
        print('非线性问题往往不好求解,所以希望能用解线性分类问题的方法解决这个问题.',
            '所采取的方法是进行一个非线性变换,将非线性问题变换为线性问题,通过解变换后的线性问题的方法求解原来的非线性问题.',
            '通过变换,左图中的椭圆变换成右图中的直线,将非线性分类问题变换为线性分类问题.',
            '通过变换,将左图中的椭圆变换成右图中的直线,将非线性分类问题变换为线性分类问题.')
        print('设原空间为X∈R^2,x=(x(1),x(2))∈X,新空间为Z∈R^2,z=(z(1),z(2))^T∈Z,',
            '定义从原空间到新空间的变换(映射):  z=phi(x)=(x(1)^2,x(2)^2)^T,',
            '经过变换z=phi(x),原空间X∈R^2变换为新空间Z∈R^2,原空间中的点相应地变换为新空间中的点,',
            '原空间中的椭圆:w1(x(1)^2,x(2)^2)^T,经过变换z=phi(x),原空间X∈R^2变换为新空间Z∈R^2,',
            '原空间中的点相应地变换为新空间中的点,原空间中的椭圆：w1(x(1))^2+w2(x(2))^2+b=0')
        print('变换成为新空间中的直线w1z(1)+w2z(2)+b=0,')
        print('变换成为新空间中的直线w1z(1)+w2z(2)+b=0')
        print('在变换后的新空间里,直线w1z(1)+w2z(2)+b=0可以将变换后的正负实例点正确分开.',
            '这样,原空间的非线性可分问题就变成了新空间的线性可分问题.')
        print('用线性分类方法求解非线性分类问题分为两步:首先使用一个变换将原空间的数据映射到新空间;',
            '然后在新空间里用线性分类学习方法从训练数据中学习分类问题.核技巧就属于这样的方法.')
        print('核技巧应用到支持向量机,其基本想法就是通过一个非线性变换将输入空间(欧式空间R^n或离散集合)',
            '对应于一个特征空间(希尔伯特空间H),使得在输入空间R^n中超曲面模型对应于特征空间H中的超平面模型(支持向量机).',
            '这样,分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成.')
        print('2.核函数的定义')
        print('定义7.6(核函数)设X是输入空间(欧式空间R^n的子集或离散集合),又设H为特征空间(希尔伯特空间),',
            '如果存在一个从X到H的映射phi(x):X->H')
        print('使得对所有x,z∈X,函数K(x,z)满足条件K(x,z)=phi(x)·phi(z)')
        print('则称K(x,z)为核函数,phi(x)为映射函数.')
        print('核技巧的想法是,在学习与预测中只定义核函数K(x,z),而不显式地定义映射函数phi.',
            '通常计算K(x,z)比较容易,而通过phi(x)和phi(z)计算K(x,z)并不容易.',
            '注意phi是输入空间R^n到特征空间H的映射,特征空间H一般是高维的,甚至是无穷维的.',
            '对于给定的核K(x,z),特征空间H和映射函数phi的取法并不唯一,可以取不同的特征空间,',
            '即便是在同一特征空间里也可以取不同的映射.')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        
chapter7 = Chapter7()

def main():
    chapter7.note()

if __name__ == '__main__':
    main()