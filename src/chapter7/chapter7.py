
class Chapter7:
    """
    第7章 支持向量机
    """
    def __init__(self):
        """
        第7章 支持向量机
        """
        pass

    def note(self):
        """
        chapter7 note
        """
        print('第7章 支持向量机')
        print('支持向量机(support vector machines, SVM)是一种二类分类器.它的基本模型是定义在特征空间上的间隔最大的线性分类器,',
            '间隔最大使它有别于感知机；支持向量机还包括核技巧,这使它成为实质上的非线性分类器.',
            '支持向量机的学习策略就是间隔最大化,可形式化为一个求解凸二次规划(convex quadratic programming)的问题,',
            '也等价于正则化的合页损失函数的最小化问题.支持向量机的学习算法是求解凸二次规划的最优化算法.')
        print('支持向量机学习方法包含构建由简至繁的模型:线性可分支持向量机、线性支持向量机及非线性支持向量机.',
            '简单模型是复杂模型的基础,也是复杂模型的特殊情况.当训练数据线性可分时,通过硬间隔最大化,',
            '也学习一个线性的分类器,即线性支持向量机,又称为软间隔支持向量机;',
            '当训练数据线性不可分时,通过使用核技巧及软间隔最大化,学习非线性支持向量机.')
        print('当输入空间为欧式空间或离散集合、特征空间为希尔伯特空间时,',
            '核函数表示将输入从输入空间映射到特征空间得到的特征向量之间的内积',
            '通过使用核函数可以学习非线性支持向量机,等价于隐式地在高维的特征空间中学习线性支持向量机.',
            '核方法是比支持向量机更一般的机器学习方法')
        print('7.1 线性可分支持向量机与硬间隔最大化.')
        print('7.1.1 线性可分支持向量机')
        print('考虑一个二类分类问题.假设输入空间与特征空间为两个不同的空间.',
            '输入空间为欧式空间或离散集合,特征空间为欧式空间或希尔伯特空间.',
            '线性可分支持向量机、线性支持向量机假设这两个空间的元素一一对应,并将输入空间中的输入映射为特征空间中的特征向量.',
            '非线性支持向量机利用一个从输入空间到特征空间的非线性映射将输入映射为特征向量.',
            '输入都由输入空间转换到特征空间,支持向量机的学习是在特征空间进行的.')
        print('假设给定一个特征空间上的训练数据集T={(x1,y1),(x2,y2),...,(xn,yn)}')
        print('其中,xi∈X=R^n,yi∈Y={+1,-1},i=1,2,...,N,xi为第i个特征向量,也称为实例,',
            'yi为xi的类标记,当yi=+1时,称xi为正例;当yi=-1时,称xi为负例,',
            '(xi,yi)称样本点.再假设训练数据集是线性可分的.')
        # !SVM学习的目标是在特征空间中找到一个分离超平面,能将实例分到不同的类.
        print('SVM学习的目标是在特征空间中找到一个分离超平面,能将实例分到不同的类.')
        print('分离超平面对应于方程w·x+b=0,由法向量w和截距b决定,可用(w,b)来表示.')
        print('分离超平面将特征空间划分为两部分,一部分是正类,一部分是负类.',
            '法向量指向一侧为正类,另一侧为负类.')
        print('一般地,当训练数据集线性可分时,存在无穷个分离超平面可将两类数据正确分开.',
            '感知机利用误分类最小的策略,求得分离超平面,不过这时的解有无穷多个.',
            '线性可分支持向量机利用间隔最大化求最优分离超平面,这时,解是唯一的')
        print('定义7.1 (线性可分支持向量机)给定线性可分训练数据集,通过间隔最大化或等价地求解相应的',
            '凸二次规划问题学习得到的分离超平面为：w*·x+b*=0 以及相应的决策函数:f(x)=sign(w*·x+b*)',
            '称为线性可分支持向量机')
        print('7.1.2 函数间隔和几何间隔')
        print('在图7.1中,有A,B,C三个点,表示3个实例,均在分离超平面的正类一侧,预测它们的类.')
        print('一般来说,一个点距离分离超平面的远近可以表示分类预测的确信程度.在超平面wx+b=0确定的情况下',
            '|wx+b|能够相对地表示点x距离超平面的远近.而wx+b的符号与类标记y的符号是否一致能够表示分类是否正确.',
            '所以可用量y(wx+b)来表示分类的正确性及确信度,这就是函数间隔的概念.')
        print('定义7.2（函数间隔）对于给定的训练数据集T和超平面(w,b),定义超平面(w,b)关于样本点(xi,yi)的函数间隔为:',
            'yi=yi(w·wi+b).定义超平面(w,b)关于训练数据集T的函数间隔为超平面(w,b)关于T中所有样本点(xi,yi)的函数间隔之最小值,即',
            'y\'=minyi')
        print('函数间隔可以表示分类预测的正确性及确信度.但是选择分离超平面时,只有函数间隔还不够.',
            '因为只要成比例地改变w和b，例如将它们改为2w和2b,超平面并没有改变,',
            '但函数间隔却成为原来的2倍.可以对分离超平面的法向量w加某些约束,如规范化,||w||=1,使得间隔是确定的.',
            '这时函数间隔成为几何间隔(geometric margin)')
        print('图7.2给出了超平面(w,b)及其法向量w.点A表示某一实例xi,其类标记为yi=+1.',
            '点A与超平面(w,b)的距离由线段AB给出,记作yi=w/||w||·xi+b/||w||')
        print('其中,||w||为w的L2范数.这是点A在超平面正的一侧的情形.如果点A在超平面负的一侧,即yi=-1,',
            '那么点与超平面的距离yi=-(w/||w||·xi+b/||w||)')
        print('一般地,当样本点(xi,yi)被超平面(w,b)正确分类时,点xi与超平面(w,b)的距离是',
            'di=yi(w/||w||·xi+b/||w||)')
        print('定义7.3(几何间隔)对于给定的训练数据集T和超平面(w,b),定义超平面(w,b)关于样本点(xi,yi)的几何间隔为,',
            'di=yi(w/||w||·xi+b/||w||)')
        print('定义超平面(w,b)关于训练数据集T的几何间隔为超平面(w,b)',
            '关于T中所有样本点(xi,yi)的几何间隔之最小值,即d=mindi')
        print('超平面(w,b)关于样本点(xi,yi)的几何间隔一般是实例点到超平面的带符号的距离,',
            '当样本点被超平面正确分类时就是实例点到超平面的距离.')
        print('从函数间隔和几何间隔的定义可知,函数间隔和几何间隔有下面的关系:di=di/||w||;d=d/||w||')
        print('如果||w||=1,那么函数间隔和几何间隔相等.如果超平面参数w和b成比例地改变(超平面没有改变),',
            '函数间隔也按此比例改变,而几何间隔不变.')
        print('7.1.3 间隔最大化')
        print('SVM学习的基本想法是求解能够正确划分训练数据集并且几何间隔最大的分离超平面.',
            '对线性可分的训练数据集而言,线性可分分离超平面有无穷多个(等价于感知机),但是几何间隔最大的分离超平面是唯一的.',
            '这里的间隔最大化又称为硬间隔最大化.')
        print('1.最大间隔分离超平面')
        print('可以表述为一个约束最优化问题 maxd s.t. yi(w/||w||·xi+b/||w||)>=d, i=1,2,...,N')
        print('注意到最大化1/||w||和最小化0.5||w||^2是等价的,于是就得到下面的线性可分支持向量机学习的最优化问题:')
        print('min 0.5||w||^2. s.t. yi(wxi+b)-1>=0')
        print('这是一个凸二次规划(convex quadratic programming)问题.')
        print('凸优化问题是指约束最优化问题')
        print('min f(w)  s.t.gi(w)<=0 i=1,2,...,k, hi(w)=0, i=1,2,...,l')
        print('其中,目标函数f(w)和约束函数gi(w)都是R^n上的连续可微的凸函数,约束函数hi(w)是R^n上的仿射函数.')
        print('当目标函数f(w)是二次函数且约束函数gi(w)是仿射函数时,上述凸最优化问题称为凸二次规划问题.')
        print('如果求出了约束最优化问题的解w*,b*,那么就可以得到最大间隔分离超平面w*·x+b*=0及分类决策函数',
            'f(x)=sign(w*·x+b*),即线性可分支持向量机模型.')
        print('算法7.1(线性可分支持向量机学习算法----最大间隔法)')
        print('输入:线性可分训练数据集T={(x1,y1),(x2,y2),...,(xn,yn)},其中,xi∈X=R^n,yi∈Y={-1,+1},i=1,2,...,N;')
        print('输出:最大间隔分离超平面和分类决策函数')
        print('(1) 构造并求解约束最优化问题:min 0.5||w||^2 s.t. yi(w·xi+b)-1>=0, i=1,2,...,N',
            '求得最优解w*,b*')
        print('(2) 由此得到分离超平面w*·x+b*=0,分类决策函数f(x)=sign(w*·x+b*)')
        print('2.最大间隔分离超平面的存在唯一性.')
        print('线性可分训练数据集的最大间隔分离超平面是存在且唯一的.')
        print('定理7.1 (最大分隔分离超平面的存在唯一性) 若训练数据集T线性可分,则可将训练数据集中的样本点',
            '完全正确分开的最大间隔分离超平面存在且唯一.')
        print('3.支持向量和间隔边界')
        print('在线性可分情况下,训练数据集的样本点中与分离超平面距离最近的样本点的实例称为支持向量.',
            '支持向量是使约束条件等号成立的点,即:yi(w·xi+b)-1=0')
        print('对yi=+1的正例点,支持向量在超平面H1:w·x+b=1上')
        print('对yi=-1的负例点,支持向量在超平面H2:w·x+b=-1上')
        print('在H1和H2上的点就是支持向量')
        print('注意到H1和H2平行,并且米有实例点落在它们中间.在H1与H2之间形成一条长带,分离超平面与它们平行且位于它们中央.',
            '长带的宽度,即H1与H2之间的距离称为间隔.间隔依赖于分离超平面的法向量w,等于2/||w||.',
            'H1和H2称为间隔边界')
        print('在决定分离超平面只有支持向量起作用,而其他实例点并不起作用.如果移动支持向量将改变所求的解;',
            '但是如果在间隔边界以外移动其他实例点,甚至去掉这些点,则解不会改变.')
        print('由于支持向量在确定分离超平面中起着决定性作用,所以将这种分类模型称为支持向量机.',
            '支持向量的个数一般很少,所以支持向量机由很少的“重要的”训练样本确定.')
        print('例7.1 数据与例2.1相同.已知一个训练集,其实例点是x1=(3,3)^T,x2=(4,3)^T,负实例点是x3=(1,1)^T,',
            '试求最大间隔分离超平面.')
        print('解,按照算法7.1,根据训练数据集构造约束最优化问题:')
        print('min 0.5(w1^2+w2^2) s.t. 3w1+3w2+b>=1 4w1+3w2+b>=1 -w1-w2-b>=1')
        print('求得此最优化问题的解w1=w2=0.5, b=-2. 于是最大间隔分离超平面为0.5x(1)+0.5x(2)-2=0',
            '其中,x1=(3,3)^T与x3=(1,1)^T为支持向量.')
        print('7.1.4 学习的对偶算法')
        print('为了求解线性可分支持向量机的最优化问题,将它作为原始最优化问题,应用拉格朗日对偶性,',
            '通过求解对偶问题得到原始问题的最优解,这就是线性可分支持向量机的对偶算法.')
        print('优点：对偶问题往往更容易求解；自然引入核函数,进而推广到非线性分类问题.')
        print('首先构建拉格朗日函数.为此,对每一个不等式约束引进拉格朗日乘子ai>=0,i=1,2,...,N,定义拉格朗日函数：')
        print('L(w,b,a)=0.5||w||^2-∑aiyi(w·xi+b)+∑ai')
        print('其中,a=(a1,a2,...,aN)^T为拉格朗日乘子向量.')
        print('根据拉格朗日对偶性,原始问题的对偶问题是极大极小问题：maxminL(w,b,a)')
        print('(1) minL(w,b,a)')
        print('  将拉格朗日函数L(w,b,a)分别对w,b求偏导并令其等于0.')
        print('(2) 求minL(w,b,a)对a的极大,即是对偶问题')
        print('  并且将目标函数由求极大转换为求极小,就得到等价的对偶最优化问题')
        print('综上所述,对于给定的线性可分训练数据集,可以首先求对偶问题的解a*,',
            '再利用公式求得原始问题的解w*,b*;从而得到分离超平面及分类决策函数.',
            '这种算法称为线性可分支持向量机的对偶学习算法,',
            '是线性可分支持向量机学习的基本算法.')
        print('算法7.2 (线性可分支持向量机学习算法)')
        print('输入:线性可分训练集T={(x1,y1),(x2,y2),...,(xn,yn)},其中xi∈X=R^n,',
            'yi∈Y={-1,+1},i=1,2,...,N;')
        print('输出:分离超平面和分类决策函数.')
        print('(1) 构造并求解约束最优化问题')
        print('  min 0.5∑∑aiajyiyj(xi·xj)-∑ai s.t. ∑aiyi=0 ai>=0,i=1,2,...,N,',
            '求得最优解a*=(a1*,a2*,...,aN*)^T')
        print('(2) 计算 w*=∑ai*yixi 并选择a*的一个正分量aj*>0,计算 b*=yj-∑aiyi(xi·xj)')
        print('(3) 求得分离超平面 w*·x+b*=0')
        print('分类决策函数:f(x)=sign(w*·x+b*)')
        print('在线性可分支持向量机中,w*和b*只依赖于训练数据中对应于ai*>0的样本点(xi,yi),',
            '而其他样本点对w*和b*没有影响.将训练数据中对应于ai*>0的实例点xi∈R^n称为支持向量.')
        print('在线性可分支持向量机中,w*和b*只依赖于训练数据中对应于ai>0的样本点对w*和b*没有影响.',
            '将训练数据中对应于ai>0的实例点xi∈R^n称为支持向量.')
        print('定义7.4(支持向量)考虑原始最优化问题及对偶优化问题,',
            '将训练数据集中对应于ai*>0的样本点(xi,yi)的实例xi∈R^n称为支持向量.')
        print('根据这一定义,支持向量一定在间隔边界上.由KKT互补条件可知,',
            'ai*(yi(w*·xi+b*)-1)=0,i=1,2,...,N')
        print('对应于ai*>0的实例xi,有yi(w*·xi+b*)-1=0或w*·xi+b*=+-1')
        print('即xi一定在间隔边界上.这里的支持向量的定义与前面给出的支持向量的定义是一致的.')
        print('例7.2 训练数据与例7.1相同.正例点是x1=(3,3)^T,x2=(4,3)^T,',
            '负例点是x3=(1,1)^T')
        print('解 根据所给数据,对偶问题是:')
        print('min 0.5∑∑aiajyiyj(xi·xj)-∑ai=',
            '0.5(18a1^2+25a2^2+2a3^2+42a1a2-12a1a3-14a2a3)-a1-a2-a3')
        print('s.t. a1+a2-a3=0  a>=0, i=1,2,3 ')
        print('解这一最优化问题.将a3=a2+a1代入目标函数并记为s(a1,a2)=4a1^2+13/2a2^2+10a1a2-2a1-2a2')
        print('对a1,a2求偏导数并令其为0,易知s(a1,a2)在点(1.5,-1)^T取极值,但该点不满足约束条件a2>=0,',
            '所以最小值应在边界上达到.')
        print('当a1=0时,最小值s(0,2/13)=-2/13;当a2=0时,最小值s(0.25,0)=-0.25.',
            '于是s(a1,a2)在a1=0.25,a2=0达到最小,此时a3=a1+a2=0.25')
        print('这样,a1*=a2*=0.25对应的实例点x1,x3是支持向量.w1*=w2*=0.5, b*=-2')
        print('分离超平面为:0.5x(1)+0.5x(2)-2=0')
        print('分离决策函数为:f(x)=sign(0.5x(1)+0.5x(2)-2)')
        print('对于线性可分问题,上述线性可分支持向量机的学习(硬间隔最大化)算法是完美的.',
            '但是,训练数据集线性可分是理想的情形.在现实问题中,训练数据集往往是线性不可分的,',
            '即在样本中出现噪声或特异点.此时,有更一般的学习算法.')
        print('7.2 线性支持向量机与间隔最大化.')
        print('7.2.1 线性支持向量机')
        print('线性可分问题的支持向量机学习方法,对线性不可分训练数据是不适用的,',
            '因为这时上述方法的不等式约束并不能都成立.就需要修改硬间隔最大化,使其成为软间隔最大化.')
        print('假设给定一个特征空间上的训练数据集T={(x1,y1),(x2,y2),...,(xn,yn)}')
        print('其中,xi∈X=R^n,yi∈Y={+1,-1},i=1,2,...,N,xi为第i个特征向量,yi为xi的类标记.',
            '再假设训练数据集不是线性可分的.通常情况是,训练数据中有一些特异点(outlier),',
            '将这些特异点除去后,剩下大部分的样本点组成的集合是线性可分的')
        print('线性不可分意味着某些样本点(xi,yi)不能满足函数间隔大于等于1的约束条件.',
            '为了解决这个问题,可以对每个样本点(xi,yi)引进一个松弛变量fi>=0',
            '使函数间隔加上松弛变量大于等于1.这样,约束条件变为:yi(w·xi+b)>=1-fi')
        print('同时,对每个松弛变量fi,支付一个代价fi.目标函数由原来的0.5||w||^2变成0.5||w||^2+C∑fi')
        print('这里,C>0称为惩罚参数,一般由应用问题决定,C值大时对误分类的惩罚增大,',
            'C值小时对误分类的惩罚减小.最小化目标函数包含两层含义：使0.5||w||^2尽量小即间隔尽量大,',
            '同时使误分类点的个数尽量小,C是调和二者的系数.')
        print('有了上面的思路,可以和训练数据集线性可分时一样来考虑训练数据集线性不可分时的线性支持向量机学习问题.',
            '相应于硬间隔最大化,称为软间隔最大化.')
        print('线性不可分的线性支持向量机的学习问题变成如下凸二次规划问题(原始问题):')
        print('min 0.5||w||^2+C∑fi  s.t. yi(w·xi+b)>=1-fi,i=1,2,...,N fi>=0,i=1,2,...,N')
        print('原始问题是一个凸二次规划问题,因而关于(w,b,f)的解是存在的.可以证明w的解是唯一的,但b的解不唯一,',
            'b的解存在于一个区间.')
        print('设原始问题的解是w*,b*,于是可以得到分离超平面w*·x+b*=0及分类决策函数',
            'f(x)=sign(w*·x+b*).称这样的模型为训练样本线性不可分时的线性支持向量机,',
            '简称为线性支持向量机.显然,线性支持向量机包含线性可分支持向量机.',
            '由于现实中训练数据往往是线性不可分的,线性支持向量机具有更广的适用性.')
        print('定义7.5(线性支持向量机) 对于给定的线性不可分的训练数据集,通过求解凸二次规划问题,',
            '即软间隔最大化问题,得到的分离超平面为:w*·x+b*=0以及相应的分类决策函数f(x)=sign(w*·x+b*)')
        print('称为线性支持向量机')
        print('7.2.2 学习的对偶算法')
        print('原始问题的对偶问题是:')
        print('min 0.5∑∑aiajyiyj(xi·xj)-∑ai s.t. ∑aiyi=0 0<=ai<=C,i=1,2,...,N')
        print('原始最优化问题的拉格朗日函数是:')
        print('L(w,b,f,a,u)=0.5||w||^2+C∑fi-∑ai(yi(w·xi+b)-1+fi)-∑uifi, 其中ai>=0,ui>=0')
        print('对偶问题是拉格朗日函数的极大极小问题.首先求L(w,b,f,a,u)对w,b,f的极小,由')
        print('grad(L(w,b,f,a,u))=w-∑aiyixi=0')
        print('grad(L(w,b,f,a,u))=-∑aiyi=0')
        print('grad(L(w,b,f,a,u))=C-ai-ui=0')
        print('得')
        print('w=∑aiyixi;  ∑aiyi=0;  C-ai-ui=0')
        print('得 minL(w,b,f,a,u)=-0.5∑∑aiajyiyj(xi·xj)+∑ai')
        print('再对minL(w,b,f,a,u)求a的极大,即得对偶问题：')
        print('max -0.5∑∑aiajyiyj(xj·xi)+∑ai')
        print('s.t. ∑aiyi=0;  C-ai-ui=0  ai>=0  ui>=0,i=1,2,...,N')
        print('可以通过求解对偶问题而得到原始的解,进而确定分离超平面和决策函数.为此,',
            '就可以定理的形式叙述原始问题的最优解和对偶问题的最优解的关系.')
        print('定理7.3 设a*=(a1*,a2*,...,aN*)^T是对偶问题的一个解,若存在a*的一个分量aj*,0<aj*<C,',
            '则原始问题的解w*,b*可按下式求得;')
        print('  w*=∑ai*yixi;   b*=yj-∑yiai*(xi·xj)')
        print('算法7.3 (线性支持向量机学习算法)')
        print('输入:训练数据集T={(x1,y1),(x2,y2),...,(xn,yn)},其中,xi∈X=R^n,yi∈Y={-1,+1},i=1,2,..,N',
            '输出:分离超平面和分类决策函数.')
        print('(1) 选择惩罚函数C>0,构造并求解凸二次规划问题')
        print('  min 0.5∑∑aiajyiyj(xi·xj)-∑ai s.t. ∑aiyi=0 0<=ai<=C, i=1,2,...,N')
        print('  求得最优解a*=(a1*,a2*,...,aN*)^T')
        print('(2) 计算w*=∑ai*yixi,选择a*的一个分量aj*适合条件0<aj*<C,计算b*=yi-∑yiai*(xi·xj)')
        print('(3) 求得分离超平面w*·x+b*=0')
        print('  分类决策函数:f(x)=sign(w*·x+b*)')
        print('步骤(2)中,对任一适合条件0<aj*<C的aj*,按上式都可求出b*,但是由于原始问题对b的解并不唯一,',
            '所以实际计算时可以取在所有符合条件的样本点上的平均值')
        print('7.2.3 支持向量')
        print('在线性不可分的情况下,将对偶问题的解a*=(a1*,a2*,...,aN*)^T中对应于ai*>0的样本点(xi,yi)',
            '的实例xi称为支持向量(软间隔的支持向量).这时的支持向量要比线性可分时的情况复杂一些.',
            '图中,分离超平面由实线表示,间隔边界由虚线表示,正实例点由“。”表示,负实例点由“×”表示.',
            '图中还标出了实例xi到间隔边界的距离fi/||w||.')
        print('软间隔的支持向量xi或者在间隔边界上,或者在间隔边界与分离超平面之间,',
            '或者在分离超平面误分一侧.若ai*<C,则fi=0,支持向量xi恰好落在间隔边界上;',
            '若ai*=C,0<fi<1,则分类正确,xi在间隔边界与分离超平面之间;',
            '若ai*=C,fi=1,则xi在分离超平面;若ai*=C,fi>1,则xi位于分离超平面误分一侧.')
        print('7.2.4 合页损失函数.')
        print('对于线性支持向量机学习来说,其模型为分离超平面w*·x+b*=0及决策函数f(x)=sign(w*·x+b*),',
            '其学习策略为软间隔最大化,学习算法为凸二次规划.')
        print('线性支持向量机学习还有另外一种解释,就是最小化以下目标函数:',
            '∑[1-yi(w·xi+b)]+la||w||^2')
        print('目标函数的第1项是经验损失或经验风险,函数')
        print('L(y(w·x+b))=[1-y(w·x+b)]+称为合页损失函数(hinge loss function)',
            '下标+表示一下取正值的函数:[z]+ =z,z>0; [z]+ =0,z<=0')
        print('这就是说,当样本点(xi,yi)被正确分类且函数间隔(确信度) yi(w·xi+b)大于1时,损失时0,',
            '否则损失是1-yi(w·xi+b),注意在图7.5中的实例点x4被正确分类,但损失不是0.',
            '目标函数第2项是系数为d的w的L2范数,是正则化项.')
        print('定理7.4 线性支持向量机原始问题最优化问题:')
        print('min 0.5||w||^2+C∑fi. s.t. yi(w·xi+b)>=1-fi, i=1,2,...,N fi>=0,i=1,2,...,N. fi>=0,i=1,2,...,N')
        print('等价于最优化问题 min ∑[1-yi(w·xi+b)]+ +d||w||^2')
        print('可将最优化问题写成问题,令1-yi(w·xi+b)=fi,fi>=0.')
        print('则yi(w·xi+b)>=1,于是w,b,fi满足约束条件.有：',
            '[1-yi(w·xi+b)]+=[fi]+=fi,所以最优化问题可以写成:',
            'min ∑fi+d||w||^2,若取d=1/2C,则',
            'min 1/C(1/2||w||^2+C∑fi)')
        print('7.3 非线性支持向量机与核函数')
        print('对解线性分类问题,线性分类支持向量机是一种非常有效的方法.但是,',
            '优势分类问题是非线性的,这时可以使用非线性支持向量机.',
            '本节叙述非线性支持向量机,其主要特点是利用核技巧.',
            '核技巧不仅应用于支持向量机,而且应用于其他统计学习问题.')
        print('7.3.1 核技巧')
        print('1.非线性分类问题')
        print('非线性分类问题是指通过利用非线性模型才能很好地进行分类的问题.')
        print('一般来说,对给定的一个训练数据集T={(x1,y1),(x2,y2),...,(xn,yn)},其中,',
            '实例xi属于输入空间,xi∈X=R^n,对应的标记有两类yi∈Y={-1,+1},i=1,2,...,N',
            '如果能用R^n中的一个超曲面将正负例正确分开,则称这个问题为非线性可分问题.')
        print('非线性问题往往不好求解,所以希望能用解线性分类问题的方法解决这个问题.',
            '所采取的方法是进行一个非线性变换,将非线性问题变换为线性问题,通过解变换后的线性问题的方法求解原来的非线性问题.',
            '通过变换,左图中的椭圆变换成右图中的直线,将非线性分类问题变换为线性分类问题.',
            '通过变换,将左图中的椭圆变换成右图中的直线,将非线性分类问题变换为线性分类问题.')
        print('设原空间为X∈R^2,x=(x(1),x(2))∈X,新空间为Z∈R^2,z=(z(1),z(2))^T∈Z,',
            '定义从原空间到新空间的变换(映射):  z=phi(x)=(x(1)^2,x(2)^2)^T,',
            '经过变换z=phi(x),原空间X∈R^2变换为新空间Z∈R^2,原空间中的点相应地变换为新空间中的点,',
            '原空间中的椭圆:w1(x(1)^2,x(2)^2)^T,经过变换z=phi(x),原空间X∈R^2变换为新空间Z∈R^2,',
            '原空间中的点相应地变换为新空间中的点,原空间中的椭圆：w1(x(1))^2+w2(x(2))^2+b=0')
        print('变换成为新空间中的直线w1z(1)+w2z(2)+b=0,')
        print('变换成为新空间中的直线w1z(1)+w2z(2)+b=0')
        print('在变换后的新空间里,直线w1z(1)+w2z(2)+b=0可以将变换后的正负实例点正确分开.',
            '这样,原空间的非线性可分问题就变成了新空间的线性可分问题.')
        print('用线性分类方法求解非线性分类问题分为两步:首先使用一个变换将原空间的数据映射到新空间;',
            '然后在新空间里用线性分类学习方法从训练数据中学习分类问题.核技巧就属于这样的方法.')
        print('核技巧应用到支持向量机,其基本想法就是通过一个非线性变换将输入空间(欧式空间R^n或离散集合)',
            '对应于一个特征空间(希尔伯特空间H),使得在输入空间R^n中超曲面模型对应于特征空间H中的超平面模型(支持向量机).',
            '这样,分类问题的学习任务通过在特征空间中求解线性支持向量机就可以完成.')
        print('2.核函数的定义')
        print('定义7.6(核函数)设X是输入空间(欧式空间R^n的子集或离散集合),又设H为特征空间(希尔伯特空间),',
            '如果存在一个从X到H的映射phi(x):X->H')
        print('使得对所有x,z∈X,函数K(x,z)满足条件K(x,z)=phi(x)·phi(z)')
        print('则称K(x,z)为核函数,phi(x)为映射函数.')
        print('核技巧的想法是,在学习与预测中只定义核函数K(x,z),而不显式地定义映射函数phi.',
            '通常计算K(x,z)比较容易,而通过phi(x)和phi(z)计算K(x,z)并不容易.',
            '注意phi是输入空间R^n到特征空间H的映射,特征空间H一般是高维的,甚至是无穷维的.',
            '对于给定的核K(x,z),特征空间H和映射函数phi的取法并不唯一,可以取不同的特征空间,',
            '即便是在同一特征空间里也可以取不同的映射.')
        print('例7.3 假设输入空间是R^2,核函数是K(x,z)=(x·z)^2,试找出其相关的特征空间H和映射phi(x):R^2->H')
        print('解：取特征空间H=R^3,记x=(x(1),x(2))^T,z=(z(1),z(2))^T,',
            '由于(x,z)^2=(x(1)z(1)+x(2)z(2))^2=(x(1)z(1))^2+2x(1)z(1)x(2)z(2)+(x(2)z(2))^2')
        print('所以可以取映射phi(x)=((x(1)^2,sqrt(2)x(1)x(2),(x(2))^2)^T')
        print('容易验证phi(x)phi(z)=(x·z)^2=K(x,z)')
        print('仍取H=R^3以及phi(x)=1/sqrt(2)*((x(1))^2-(x(2))^2, 2x(1)x(2),(x(1))^2+(x(2))^2)^T')
        print('同样有phi(x)·phi(z)=(x·z)^2=K(x,z)')
        print('还可以取H=R^4和phi(x)=((x(1)^2,x(1)x(2),x(1)x(2),(x(2)^2))^T')
        print('3.核技巧在支持向量机中的应用')
        print('注意到在线性支持向量机的对偶问题中,无论是目标函数还是决策函数(分离超平面)都只涉及输入实例与实例之间的内积.',
            '在对偶问题的目标函数中的内积xi·xj可以用核函数K(xi,xj)=phi(xi)·phi(xj)来代替.',
            '此时对偶问题的目标函数成为:W(a)=0.5∑∑aiajyiyjK(xi,xj)-∑ai')
        print('同样,分类决策函数中的内积也可以用核函数代替,而分类决策函数式成为:')
        print(' f(x)=sign(∑aiyiphi(xi)·phi(x)+b*)=sign(∑aiyiK(xi,x)+b*)')
        print('这等价于经过映射函数phi将原来的输入空间变换到一个新的特征空间,',
            '将输入空间中的内积xi·xj变换为特征空间中的内积phi(xi)·phi(xj),',
            '在新的特征空间里面从训练样本中学习线性支持向量机.',
            '当映射函数是非线性函数时,学习到的含有核函数的支持向量机是非线性分类模型.')
        print('就是说,在核函数K(x,z)给定的条件下,可以利用解线性分类问题的方法求解非线性分类问题的支持向量机.',
            '学习是隐式地在特征空间进行的,不需要显式地定义特征空间和映射函数')
        print('7.3.2 正定核')
        print('已知映射函数phi,可以通过phi(x)和phi(z)的内积求得核函数K(x,z).不用构造映射phi(x)能否直接判断一个给定的函数K(x,z)是不是核函数?',
            '或者说,函数K(x,z)满足什么条件才能成为核函数.')
        print('假设K(x,z)是定义在X*X上的对称函数,并且对任意的x1,x2,...,xm∈X,K(x,z)关于x1,x2,...,xm的Gram矩阵是半正定的.',
            '可以依据函数K(x,z),构成一个希尔伯特空间(Hilbert space),其步骤是:首先定义映射phi并构成向量空间S;',
            '然后在S上定义内积构成内积空间;最后将S完备化构成希尔伯特空间')
        print('1.定义映射,构成向量空间S')
        print('先定义映射phi:x->K(·,x).',
            '根据这一映射,对任意的xi∈X，ai∈R,i=1,2,...,m,定义线性组合f(·)=∑aiK(·,xi)')
        print('考虑由线性组合为元素的集合S.由于集合S对加法和数乘运算是封闭的,所以S构成一个向量空间.')
        print('2.在S上定义一个运算*,对任意f,g∈S,',
            'f(·)=∑aiK(·,xi);g(·)=∑bjK(·,zj)')
        print('定义运算*,f*g·∑∑aibjK(xi,zj)')
        print('证明运算*是空间S的内积.为此要证:')
        print('(1) (cf)*g=c(f*g),c∈R')
        print('(2) (f+g)*h=f*h+g*h,h∈S')
        print('(3) f*g=g*f')
        print('(4) f*f>=0')
        print('  f*f=0 <=> f=0')
        print('3.将内积空间S完备为希尔伯特空间')
        print('  由定义的内积可以得到范数||f||=sqrt(f·f)')
        print('因此,S是一个赋范向量空间.根据泛函分析理论,对于不完备的赋范向量空间S,',
            '一定可以使之完备化,得到完备的赋范向量空间H.一个内积空间,当做一个赋范向量空间是完备的时候,',
            '就是希尔伯特空间H')
        print('这一希尔伯特空间H称为再生核希尔伯特空间.这是由于核K具有再生性,即满足K(·,x)·f=f(x)')
        print('及K(·,x)·K(·,z)=K(x,z)称为再生核')
        print('4.正定核的充要条件')
        print('定理7.5 (正定核的充要条件)设K:X*X->R是对称函数,则K(x,z)为正定核函数的充要条件是对任意xi∈X,',
            'i=1,2,...,m,K(x,z)对应的Gram矩阵K=[K(xi,xj)]m*m是半正定矩阵.')
        print('定义7.7（正定核的等价定义）设X∈R^n,K(x,z)是定义在X*X上的对称函数,如果对任意xi∈X,i=1,2,...,m',
            'K(x,z)对应的Gram矩阵K=[K(xi,xj)]m*m是半正定矩阵,则称K(x,z)是正定核')
        print('这一定义在构造核函数时很有用,但对于一个具体函数K(x,z)来说,检验它是否为正定核函数并不容易,',
            '因为要求对任意有限输入集{x1,x2,...,xm}验证K对应的Gram矩阵是否为半正定的.',
            '在实际问题中往往应用已有的核函数.另外,由Mercer定理可以得到Mercer核,正定核比Mercer核更具有一般性')
        print('7.3.3 常用核函数')
        print('1.多项式核函数(polynomial kernel function)',
            'K(x,z)=(x·z+1)^p,对应的支持向量机是一个p次多项式分类器.',
            '在此情形下,分类决策函数成为:f(x)=sign(∑aiyi(xi·x+1)^p+b*)')
        print('2.高斯核函数(Gaussian kernel function)',
            'K(x,z)=exp(-||x-z||^2/2o^2)',
            '对应的支持向量机是高斯径向基函数分类器.在此情形下,分类决策函数成为:',
            'f(x)=sign(∑aiyiexp(-||x-z||^2/2o^2)+b*)')
        print('3.字符串核函数(string kernel function)')
        print('核函数不仅可以定义在欧式空间上,还可以定义在离散数据的集合上.',
            '比如字符串核是定义在字符串集合上的核函数.字符串核函数在文本分类、信息检索、生物信息学等方面都有应用')
        print('考虑一个有限字符表∑.字符串s是从∑中取出的有限个字符的序列,包括空字符串.',
            '字符串s的长度用|s|表示,它的元素记作s(1)s(2)...s(|s|).两个字符串s和t的连接记作st.',
            '所有长度为n的字符串集合记作∑^n,所有字符串的集合记作∑^*=∪∑^n')
        print('考虑字符串s的子串u.给定一个指标序列i=(i1,i2,...,iu),1<=i1<i2<...<iu<=|s|,',
            's的子串定义为u=s(i)=s(i1)s(i2)...s(iu),其长度记作l(i)=iu-i1+1.',
            '如果i是连续的,则l(i)=|u|;否则,l(i)>|u|.')
        print('假设S是长度大于或等于n字符串的集合,s是S的元素.',
            '现在建立字符串集合S到特征空间Hn=R^(∑^n)的映射phin(s).',
            'R^(∑^n)表示定义在∑^n上的实数空间,其每一维对应一个字符串u∈∑^n,',
            '映射phin(s)将字符串s对应于空间R^(∑^n)的一个向量,其在u维上的取值为:',
            '[phin(s)]u=∑l^l(i)',
            '这里,0<l<=1是一个衰减参数,l(i)表示字符串i的长度,求和在s中所有与u相同的子串上进行.')
        print('例如,假设∑为英文字符集,n为3,S为长度大于或等于3的字符串的集合.',
            '考虑将字符串S映射到特征空间H3,H3的一维对应于字符串asd.')
        print('两个字符串s和t上的字符串核函数是基于映射phin的特征空间中的内积:')
        print('  kn(s,t)=∑[phin(s)][phin(t)]=∑∑l^l(i)l^l(j)')
        print('字符串核函数kn(s,t)给出了字符串s和t中长度等于n的所有子串组成的特征向量的余弦相似度.',
            '直观上,两个字符串相同的子串越多,它们就越相似,字符串函数的值就越大.',
            '字符串核函数可以由动态规划快速地计算.')
        print('7.3.4 非线性支持向量分类机')
        print('利用核技巧,可以将线性分类的学习方法应用到非线性分类问题中去.',
            '将线性支持向量机扩展到非线性支持向量机,只需将线性支持向量机对偶形式中的内积换成核函数.')
        print(' f(x)=sign(∑aiyiK(x,xi)+b*)称为非线性支持向量,K(x,z)是正定核函数.')
        print('算法7.4（非线性支持向量机学习算法）')
        print('输入:训练数据集T={(x1,y1),(x2,y2),...,(xn,yn)},其中xi∈X=R^n,yi∈Y={-1,+1},i=1,2,...N;')
        print('输出:分类决策函数.')
        print('(1) 选取适当的核函数K(x,z)和适当的参数C,构造并求解最优化问题')
        print('  min0.5∑∑aiajyiyjK(xi,xj)-∑ai  s.t. ∑aiyi=0, 0<=ai<=C, i=1,2,...,N',
            '求得最优解a*=(a1*,a2*,...,aN*)^T.')
        print('(2) 选择a*的一个正分量0<aj*<C,计算:',
            'b*=yj-∑aiyiK(xi·xj)')
        print('(3) 构造决策函数:')
        print('  f(x)=sign(∑aiyiK(x·xi)+b*)')
        print('当K(x,z)是正定核函数,凸二次规划问题,解是存在的.')
        print('7.4 序列最小最优化算法.')
        # !支持向量机的学习问题可以形式化为求解凸二次规划问题.这样的凸二次规划问题具有全局最优解
        print('支持向量机的学习问题可以形式化为求解凸二次规划问题.这样的凸二次规划问题具有全局最优解',
            '但是当训练样本容量很大时,这些算法往往变得非常低效,以致无法使用.',
            '所以,如何高效地实现支持向量机学习问题就成为一个重要的问题.')
        print('支持向量机前人提出了许多快速算法,如序列最小最优化算法.')
        print('SMO算法要解如下凸二次规划的对偶问题:',
            'min0.5∑∑aiajyiyjK(xi,xj)-∑ai s.t. ∑aiyi=0 0<=ai<C,i=1,2,...,N')
        print('在这个问题中,变量是拉格朗日乘子,一个变量ai对应于一个样本点(xi,yi);',
            '变量的总数等于训练样本容量N.')
        print('SMO算法是一种启发式算法,其基本思路是:如果所有变量的解都满足此最优化问题的KKT条件,',
            '那么这个最优化问题的解就得到了.因为KKT条件是该最优化问题的充分必要条件.',
            '否则,选择两个变量,固定其他变量,针对这两个变量构建一个二次规划问题.')
        print('这个二次规划问题关于这两个变量的解应该更接近原始二次规划问题的解,',
            '因为这会使得原始二次规划问题的目标函数值变得更小.',
            '重要的是,这时子问题可以通过解析方法求解,这样就可以大大提高整个算法的计算速度.',
            '子问题有两个变量,一个违反KKT条件最严重的那一个,另一个由约束条件自动确定.',
            '如此,SMO算法将原问题不断分解为子问题并对子问题求解,进而达到求解原问题的的目的.')
        print('注意,子问题的两个变量中只有一个是自由变量.假设a1,a2为两个变量,a3,a4,...,aN固定',
            '那么由等式约束可知a1=-y1∑aiyi.如果a2确定,那么a1也随之确定.所以子问题中同时更新两个变量.')
        print('整个SMO算法包括两个部分:求解两个变量二次规划的解析方法和选择变量的启发式方法.')
        print('7.4.1 两个变量二次规划的求解方法')
        print('不失一般性,假设选择的两个变量是a1,a2,其他变量ai(i=3,4,...,N)是固定的.',
            '于是SMO的最优化问题的子问题都可以写成:',
            'min W(a1,a2)=0.5K11a1^2+0.5K22a2^2+y1y2K12a1a2-',
            '(a1+a2)+y1a1∑yiaiKi1+y2a2∑yiaiKi2')
        print('s.t. a1y2+a2y2=-∑yiai=u 0<=ai<=C, i=1,2')
        print('其中,Kij=K(xi,xj),i,j=1,2,...,N. u是常数,目标函数式中省略了不含a1,a2的常数项')
        print('为了求解两个变量的二次规划问题,首先分析约束条件,然后在此约束条件下求极小.')
        print('由于只有两个变量a1,a2;约束可以用二维空间中的图形表示.')
        print('不等式约束使得(a1,a2)在盒子[0,C]*[C,0]内,等式约束使(a1,a2)在平行于盒子[0,C]*[C,0]的对角线的直线上.',
            '因此要求的是目标函数在一条平行于对角线线段上的最优值.',
            '这使得两个变量的最优化问题成为实质上的单变量的最优化问题,',
            '不妨考虑为变量a2的最优化问题.')
        print('假设问题的初始可行解为a1old,a2old,最优解为a1new,a2new,',
            '并且假设在沿着约束方向未经剪辑时a2的最优解为a2newtime')
        print('由于a2new需满足不等式约束,所以最优值a2new的取值范围必须满足条件,',
            'L<=a2new<=H')
        print('其中,L与H是a2new所在的对角线段端点的界.如果y1!=y2,',
            'L=max(0,a2old-a1old), H=min(C,C+a2old-a1old)')
        print('如果y1=y2,则:',
            'L=max(0,a2old-a1old), H=min(C,a2old+a1old)')
        print('下面,首先求沿着约束方向未经剪辑即未考虑不等式约束时a2的最优解a2newtime;',
            '然后再求剪辑后a2的解a2new.用定理来叙述这个结果.为了叙述简单,记',
            'g(x)=∑aiyiK(xi,x)+b')
        print('令Ei=g(xi)-yi=(∑ajyjK(xj,xi)+b)-yi,i=1,2')
        print('定理7.6 最优化问题沿着约束方向未经剪辑时的解是a2new,unc=a2old+y2(E1-E2)/y')
        print('其中,y=K11+K22-2K12=||P(x1)-P(x2)||^2,')
        print('P(x)是输入空间到特征空间的映射,Ei=1,2,由式给出:')
        print('要使其满足不等式约束必须其限制在区间[L,H]内,从而得到a2new的表达式.',
            '于是得到最优化问题的解(a1new,a2new)')
        print('7.4.2 变量的选择方法')
        print('1.第1个变量的选择')
        print('SMO称选择第1个变量的过程为外层循环.外层循环在训练样本中选取违反KKT条件最严重的样本点,',
            '并将其对应的变量作为第1个变量.具体地,检验训练样本点(xi,yi)是否满足KKT条件,即',
            'ai=0 <=> yigi(xi)>=1   0<ai<C<=>yig(xi)=1   ai=C<=>yig(xi)<=1')
        print('该检验是在e范围内进行的.在检验过程中,外层循环首先遍历所有满足条件0<ai<C的样本点',
            '即在间隔边界上的支持向量点,检验它们是否满足KKT条件.如果这些样本点都满足KKT条件,',
            '那么遍历整个训练集,检验它们是否满足KKT条件')
        print('2.第2个变量的选择')
        print('SMO称选择第2个变量的过程为内层循环.假设在外层循环中已经找到第1个变量a1,',
            '现在要在内层循环中找第2个变量a2.第2个变量选择的标准是希望能使a2有足够大的变化')
        print('a2new是依赖于|E1-E2|的,为了加快计算速度,一种简单的做法是选择a2,使其对应的|E1-E2|最大'.
            '因为a1已定,E1也确定了.如果E1是正的,那么选择最小的E1作为E2;如果E1是负的,',
            '那么选择最大的E1作为E2.为了节省计算时间,将所有E1值保存在一个列表中.')
        print('在特殊情况下,如果内层循环通过以上方法选择的a2不能使目标函数有足够的下降,',
            '那么采用以下启发式规则继续选择a2,遍历在间隔边界上的支持向量点,',
            '依次将其对应的变量作为a2试用,直到目标函数有足够的下降.',
            '若找不到合适的a2,那么遍历训练数据集;若仍找不到合适的a2,则放弃第1个a1,',
            '再通过外层循环寻求另外的a1')
        print('3.计算阈值b和差值Ei')
        print('在每次完成两个变量的优化后,都要重新计算阈值b.当0<a1new<C时,由KKT条件可知：',
            '∑aiyiKi1+b=yi')
        print('于是,b1new=y1-∑aiyiKi1-a1newy1K11-a2newy2K21')
        print('由E1的定义式有:',
            'E1=∑aiyiKi1+a1oldy1Ki1+a2oldy2K21+bold-y1')
        print('式的前两项可写成:',
            'y1-∑aiyiKi1=-E1+a1oldy1K11+a2oldy2K21+bold')
        print('可得:','-E1-y1K11(a1new-a1old)-y2K21(a2new-a2old)+bold')
        print('同样,如果0<a2new<C,那么,',
            'b2new=-E2-y1K12(a1new-a1old)-y2K22(a2new-a2old)+bold')
        print('如果a1new,a2new同时满足条件0<ainew<C,i=1,2,那么b1new=b2new.',
            '如果a1new,a2new是0或者C,那么b1new和b2new以及它们之间的数都是符合KKT条件的阈值,',
            '这时选择它们的中点作为bnew')
        print('在每次完成两个变量的优化之后,还必须更新对应的Ei值,并将它们保存在列表中.',
            'Ei值的更新要用到bnew值,以及所有支持向量对应的aj:')
        print('  Einew=∑yjajK(xi,xj)+bnew-yi, 其中,S是所有支持向量xj的集合')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')

chapter7 = Chapter7()

def main():
    chapter7.note()

if __name__ == '__main__':
    main()