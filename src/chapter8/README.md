# 第8章 提升方法

1.提升方法是将弱学习算法提升为强学习算法的统计学习方法. 在分类学习中,提升方法通过反复修改训练数据的权值分布,构建一系列的基本分类器(弱分类器) 并将这些基本分类器线性组合,构成一个强分类器.代表性的提升方法是AdaBoost方法
2.AdaBoostu算法的特点是通过迭代每次学习的一个基本分类器. 每次迭代中,提高那些被前一轮分类器错误分类数据的权值, 而降低那些被正确分类的数据的权值.最后,AdaBoost将基本分类器的线性组合作为强分类器, 其中给分类误差率小的基本分类器以大的权值,给分类误差率大的基本分类器以小的权值.
3.AdaBoost的训练误差分析表明,AdaBoost的每次迭代可以减少它在训练数据集上的分类误差率, 这说明了它作为提升方法的有效性.
4.AdaBoost算法的一个解释是该算法实际是前向分布算法的一个实现. 在这个方法里,模型是加法模型,损失函数是指数损失,算法是前向分布算法. 每一步中极小化损失函数(bm,ym)=argmin∑L(yi,fm-1(xi)+bmb(xi;y)) 得到参数bm,ym
5.提升树是以分类树或回归树为基本分类器的提升方法.提升树被认为是统计学习中最有效的方法之一.
决策树、SVM、AdaBoost的比较
决策树
  真实应用场景：金融方面使用决策树建模分析，用于评估用户的信用，电商推荐系统
  优势：易于实现和理解，数据准备工作简单，同时处理多种数据类型，通过静态测试来对模型表现进行评价; 可以在较短时间内对大量数据做出非常好的结果,决策树可以很好地扩展到大型数据中, 同时决策树的大小独立于数据库的大小.计算复杂度低,结果易于理解,对部分数据损失不敏感.
  表现最好的情况:实例是由“属性-值”对表示的;目标函数具有离散的输出值;训练数据集包含分布错误 (决策树对错误有适应性),训练数据缺少少量属性的实例
  缺点:易于出现过拟合问题;忽略了数据集中属性之间的相关性; 对于类比不一致的样本,决策树的信息增益倾向于那些数据值较多的特征;
  什么条件表现很差:决策树匹配数据过多时,分类的类别过于复杂;数据的树形之间具有非常强的关联
  模型适应问题:不需要准备太多的训练数据,不需要对数据过多的处理如删除空白 该问题是非线性问题,决策树能够很好地解决非线性问题.算法的执行效率高,对机器的要求小
支持向量机SVM
  真实应用场景：文本和超文本的分类,用于图像分类,用于手写体识别
  优势：分类效果好;可以有效地处理高维空间的数据,可以有效地处理变量个数大于样本个数的数据 只是使用了一部分子集来进行训练模型,所以SVM模型不需要太大的内存; 可以提高泛化能力,无局部极小值问题.
  表现最好的情况:数据的维度较高,需要模型具有非常强的泛化能力;样本数据量较小时 解决非线性问题
  缺点:无法处理大规模的数据集,算法需要的时间较长的训练时间. 无法有效地处理包含噪声太多的数据集;SVM模型没有直接给出概率的估计值,而是利用交叉验证的方式估计, 这种方式耗时较长;对缺失数据敏感;对于非线性问题,有时很难找到一个合适的核函数
  什么条件表现很差:数据集的数据量过大;数据集中含有噪声;数据集中的缺失较多的数据; 对算法的训练效率要求较高.
  模型适应问题:该项目所提供的样本数据相对较少;该问题是属于非线性问题;数据集经过“读热编码”后,维度较高
适应提升方法AdaBoost
  真实应用场景：二分类或多分类问题,用于特征选择,多标签问题,回归问题.
  优势：AdaBoost是一种精度非常高的分类器,可以与各种方法构建子分类器,Adaboost算法提供一种计算框架; 弱分类器的构造方法比较简单;算法易于理解,不用做特征筛选,不易发生过拟合.易于编码
  表现最好的情况:用于解决二分类问题,解决大类单标签问题;处理多类单标签问题;处理回归相关的问题.
  缺点:AdaBoost算法的迭代次数不好设定,需要使用交叉验证的方式来进行确定; 数据集的不平衡分布导致分类器的分类精度下降;训练比较耗费时间;对异常值比较敏感.
  什么条件表现很差:数据集分布非常不均匀,数据集中含有较多的异常值,对算法的训练的效率要求比较高
  模型适应问题:该数据集可以归属为多标签分类问题;数据集中异常值较少; 对算法模型的准确率要求较高.

