
class Chapter8:
    """
    第8章 提升方法
    """
    def __init__(self):
        """
        第8章 提升方法
        """
        pass

    def note(self):
        """
        chapter8 note
        """
        print('第8章 提升方法')
        print('提升(boosting)方法是一种常用的统计学习方法,应用广泛且有效.',
            '在分类问题中,通过改变训练样本的权重,学习多个分类器,',
            '并将这些分类器线性组合,提高分类的性能')
        print('8.1 提升方法AdaBoost算法')
        print('8.1.1 提升方法的基本思路')
        print('提升方法基于这样一种思想:对于一个复杂任务来说,',
            '将多个专家的判断进行适当的综合所得出的判断.',
            '要比其中任何一个专家单独的判断好.',
            '类似“三个臭皮匠顶个诸葛亮”')
        print('“强可学习”和“弱可学习”.',
            '在概率近似正确(PAC)学习框架中,一个概念(一个类),如果存在一个多项式的学习算法能够学习它',
            '并且正确率很高,那么就称这个概念是强可学习的;一个概念,',
            '如果存在一个多项式的学习算法能够学习它,学习的正确率仅比随机猜测略好,',
            '那么就称这个概念是弱可学习的.')
        print('在PAC学习的框架下,一个概念是强可学习的充分必要条件是这个概念是弱可学习的.')
        print('对于分类问题而言,给定一个训练样本集,求比较粗糙的分类规则(弱分类器)要比求精确的分类规则(强分类器)',
            '容易的多,提升方法就是从弱学习算法出发,反复学习,得到一系列弱分类器(又称为基本分类器),',
            '然后组合这些弱分类器,构成一个强分类器.大多数提升方法都是改变训练数据的概率分布(训练数据的权值分布)',
            '针对不同的训练数据分布调用弱学习算法学习一系列弱分类器')
        print('AdabBoost算法做法是:提高那些被前一轮弱分类器错误分类样本的权值,',
            '而降低那些被正确分类样本的权值,这样一来,那些没有得到正确分类的数据,',
            '由于其权值的加大而受到后一轮的弱分类器的更大关注.')
        print('于是,分类问题被一系列的弱分类器“分而治之”.弱分类器的组合,',
            'AdaBoost采取加权多数表决的方法.具体地,加大分类误差率小的弱分类器的权值,',
            '使其在表决中起较大的作用,减小分类误差率大的弱分类器的权值,使其在表决中起较小的作用.',
            'AdaBoost的巧妙之处就在于它将这些想法自然且有效地实现在一种算法里')
        print('8.1.2 AdaBoost算法')
        print('假设给定一个二类分类的训练数据集T={(x1,y1),(x2,y2),...,(xn,yn)}.',
            '其中,每个样本点由实例与标记组成.实例xi∈X∈R^n,标记yi∈Y={-1,+1},',
            'X是实例空间,Y是标记集合.AdaBoost利用一下算法,从训练数据中学习一系列弱分类器或基本分类器',
            '并将这些弱分类器线性组合成为一个强分类器')
        print('算法8.1 (AdaBoost)')
        print('输入:训练数据集T={(x1,y1),(x2,y2),...,(xn,yn)},其中xi∈X∈R^n,yi∈Y={-1,+1};弱学习算法;')
        print('输出:最终分类器G(x)')
        print('(1) 初始化训练数据的权值分布D1=(w11,...,w1i,...,w1N),w1i=1/N,w1i=1/N,i=1,2,...,N')
        print('(2) 对m=1,2,...,M')
        print('  (a) 使用具有权值分布Dm的训练数据集学习,得到基本分类器Gm(x)=X->{-1,+1}')
        print('  (b) 计算Gm(x)在训练数据集上的分类误差率')
        print('  (c) 计算Gm(x)的系数am=0.5log(1-em)/em')
        print('  (d) 更新训练数据集的权值分布')
        print('(3) 构建基本分类器的线性组合f(x)=∑amGm(x)')
        print('最终得到分类器:G(x)=sign(f(x))=sign(∑amGm(x))')
        print('步骤(1) 假设训练数据集具有均匀的权值分布,即每个训练样本在基本分类器的学习中作用相同,',
            '这一假设保证第1步能够在原始数据上学习基本分类器G1(x).')
        print('步骤(2) AdaBoost反复学习基本分类器,在每一轮m=1,2,...,M顺次地执行下列操作：')
        print('  (a) 使用当前分布Dm加权的训练数据集,学习基本分类器Gm(x)')
        print('  (b) 计算基本分类器Gm(x)在加权训练数据集上的分类误差率:')
        print('这里,wmi表示第m轮中第i个实例的权值,∑wmi=1.')
        print('Gm(x)在加权的训练数据集上的分类误差率是被Gm(x)误分类样本的权值之和,',
            '由此可以看出数据权值分布Dm与基本分类器Gm(x)的分类误差率的关系')
        print('  (c) 计算基本分类器Gm(x)的系数am·am表示Gm(x)在最终分类器中的重要性.',
            '当em<=0.5时,am>=0,并且am随着em的减小而增大,',
            '所以分类误差率越小的基本分类器在最终分类器的作用越大')
        print('  (d) 更新训练数据的权值分布为下一轮作准备.')
        print('由此可知,被基本分类器Gm(x)误分类样本的权值得以扩大,而被正确分类样本的权值却得以缩小.',
            '两相比较,误分类样本的权值被放大.因此,误分类样本在下一轮学习中起更大的作用.',
            '不改变所给的训练数据,而不断改变训练数据权值的分布,使得训练数据在基本分类器的学习中起不同的作用,',
            '这是AdaBoost的一个特点.')
        print('步骤(3) 线性组合f(x)实现M个基本分类器的加权表决.系数am表示了基本分类器Gm(x)的重要性',
            '这里,所有am之和并不为1.f(x)的符号决定实例x的类,f(x)的绝对值表示分类的确信度.',
            '利用基本分类器的线性组合构建最终分类器是AdaBoost的另一特点.')
        print('8.1.3 AdaBoost例子')
        print('例子8.1 给定如表8.1所示的训练数据.假设弱分类器由x<v或x>v产生,',
            '其阈值v使该分类器在训练数据集上分类误差率最低',
            '试用AdaBoost算法学习一个强分类器')
        print('解：初始化数据权值分布D1=(w11,w12,...,wl10),w1i=0.1,i=1,2,...,10.',
            '对m=1')
        print(' (a) 在权值分布为D1的训练数据上,阈值v取2.5时分类误差率最低,故基本分类器为',
            'G1(x)=1 x<2.5, G1(x)=-1 x>2.5')
        print(' (b) G1(x)在训练数据集上的误误差率e1=P(G1(xi)!=yi)=0.3')
        print(' (c) 计算G1(x)的系数:a1=0.5log(1-e1)/e1=0.4236')
        print(' (d) 更新训练数据的权值分布:D2=(w21,...,w2i,...,w110)',
            'w2i=w1i/Z1exp(-a1yiG1(xi)),i=1,2,...,10')
        print('  D2=(0.0715m0.0715m0.0715,...)')
        print('  f1(x)=0.4236G1(x)')
        print('分类器sign[f1(x)]在训练数据集上有3个误分类点')
        print('对m=2,')
        print(' (a) 在权值分布为D2的训练数据上,阈值v是8.5时分类误差率最低,基本分类器为：',
            'G2(x)=1, x<8.5; G2(x)=-1, x>8.5')
        print(' (b) G2(x)z在训练数据集上的误差率e2=0.2143')
        print(' (c) 计算a2=0.6496')
        print(' (d) 更新训练数据权值分布:D3=(...),f(2)=0.4236G1(x)+0.6496G2(x); f2(x)',
            '分类器sign[f2(x)]在训练数据集上有3个误分类点')
        print('对m=3,同理可得.')
        print('8.2 AdaBoost算法的训练误差分析.')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')

chapter8 = Chapter8()

def main():
    chapter8.note()

if __name__ == '__main__':
    main()