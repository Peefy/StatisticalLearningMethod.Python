
class Chapter8:
    """
    第8章 提升方法
    """
    def __init__(self):
        """
        第8章 提升方法
        """
        pass

    def note(self):
        """
        chapter8 note
        """
        print('第8章 提升方法')
        print('提升(boosting)方法是一种常用的统计学习方法,应用广泛且有效.',
            '在分类问题中,通过改变训练样本的权重,学习多个分类器,',
            '并将这些分类器线性组合,提高分类的性能')
        print('8.1 提升方法AdaBoost算法')
        print('8.1.1 提升方法的基本思路')
        print('提升方法基于这样一种思想:对于一个复杂任务来说,',
            '将多个专家的判断进行适当的综合所得出的判断.',
            '要比其中任何一个专家单独的判断好.',
            '类似“三个臭皮匠顶个诸葛亮”')
        print('“强可学习”和“弱可学习”.',
            '在概率近似正确(PAC)学习框架中,一个概念(一个类),如果存在一个多项式的学习算法能够学习它',
            '并且正确率很高,那么就称这个概念是强可学习的;一个概念,',
            '如果存在一个多项式的学习算法能够学习它,学习的正确率仅比随机猜测略好,',
            '那么就称这个概念是弱可学习的.')
        print('在PAC学习的框架下,一个概念是强可学习的充分必要条件是这个概念是弱可学习的.')
        print('对于分类问题而言,给定一个训练样本集,求比较粗糙的分类规则(弱分类器)要比求精确的分类规则(强分类器)',
            '容易的多,提升方法就是从弱学习算法出发,反复学习,得到一系列弱分类器(又称为基本分类器),',
            '然后组合这些弱分类器,构成一个强分类器.大多数提升方法都是改变训练数据的概率分布(训练数据的权值分布)',
            '针对不同的训练数据分布调用弱学习算法学习一系列弱分类器')
        print('AdabBoost算法做法是:提高那些被前一轮弱分类器错误分类样本的权值,',
            '而降低那些被正确分类样本的权值,这样一来,那些没有得到正确分类的数据,',
            '由于其权值的加大而受到后一轮的弱分类器的更大关注.')
        print('于是,分类问题被一系列的弱分类器“分而治之”.弱分类器的组合,',
            'AdaBoost采取加权多数表决的方法.具体地,加大分类误差率小的弱分类器的权值,',
            '使其在表决中起较大的作用,减小分类误差率大的弱分类器的权值,使其在表决中起较小的作用.',
            'AdaBoost的巧妙之处就在于它将这些想法自然且有效地实现在一种算法里')
        print('8.1.2 AdaBoost算法')
        print('假设给定一个二类分类的训练数据集T={(x1,y1),(x2,y2),...,(xn,yn)}.',
            '其中,每个样本点由实例与标记组成.实例xi∈X∈R^n,标记yi∈Y={-1,+1},',
            'X是实例空间,Y是标记集合.AdaBoost利用一下算法,从训练数据中学习一系列弱分类器或基本分类器',
            '并将这些弱分类器线性组合成为一个强分类器')
        print('算法8.1 (AdaBoost)')
        print('输入:训练数据集T={(x1,y1),(x2,y2),...,(xn,yn)},其中xi∈X∈R^n,yi∈Y={-1,+1};弱学习算法;')
        print('输出:最终分类器G(x)')
        print('(1) 初始化训练数据的权值分布D1=(w11,...,w1i,...,w1N),w1i=1/N,w1i=1/N,i=1,2,...,N')
        print('(2) 对m=1,2,...,M')
        print('  (a) 使用具有权值分布Dm的训练数据集学习,得到基本分类器Gm(x)=X->{-1,+1}')
        print('  (b) 计算Gm(x)在训练数据集上的分类误差率')
        print('  (c) 计算Gm(x)的系数am=0.5log(1-em)/em')
        print('  (d) 更新训练数据集的权值分布')
        print('(3) 构建基本分类器的线性组合f(x)=∑amGm(x)')
        print('最终得到分类器:G(x)=sign(f(x))=sign(∑amGm(x))')
        print('步骤(1) 假设训练数据集具有均匀的权值分布,即每个训练样本在基本分类器的学习中作用相同,',
            '这一假设保证第1步能够在原始数据上学习基本分类器G1(x).')
        print('步骤(2) AdaBoost反复学习基本分类器,在每一轮m=1,2,...,M顺次地执行下列操作：')
        print('  (a) 使用当前分布Dm加权的训练数据集,学习基本分类器Gm(x)')
        print('  (b) 计算基本分类器Gm(x)在加权训练数据集上的分类误差率:')
        print('这里,wmi表示第m轮中第i个实例的权值,∑wmi=1.')
        print('Gm(x)在加权的训练数据集上的分类误差率是被Gm(x)误分类样本的权值之和,',
            '由此可以看出数据权值分布Dm与基本分类器Gm(x)的分类误差率的关系')
        print('  (c) 计算基本分类器Gm(x)的系数am·am表示Gm(x)在最终分类器中的重要性.',
            '当em<=0.5时,am>=0,并且am随着em的减小而增大,',
            '所以分类误差率越小的基本分类器在最终分类器的作用越大')
        print('  (d) 更新训练数据的权值分布为下一轮作准备.')
        print('由此可知,被基本分类器Gm(x)误分类样本的权值得以扩大,而被正确分类样本的权值却得以缩小.',
            '两相比较,误分类样本的权值被放大.因此,误分类样本在下一轮学习中起更大的作用.',
            '不改变所给的训练数据,而不断改变训练数据权值的分布,使得训练数据在基本分类器的学习中起不同的作用,',
            '这是AdaBoost的一个特点.')
        print('步骤(3) 线性组合f(x)实现M个基本分类器的加权表决.系数am表示了基本分类器Gm(x)的重要性',
            '这里,所有am之和并不为1.f(x)的符号决定实例x的类,f(x)的绝对值表示分类的确信度.',
            '利用基本分类器的线性组合构建最终分类器是AdaBoost的另一特点.')
        print('8.1.3 AdaBoost例子')
        print('例子8.1 给定如表8.1所示的训练数据.假设弱分类器由x<v或x>v产生,',
            '其阈值v使该分类器在训练数据集上分类误差率最低',
            '试用AdaBoost算法学习一个强分类器')
        print('解：初始化数据权值分布D1=(w11,w12,...,wl10),w1i=0.1,i=1,2,...,10.',
            '对m=1')
        print(' (a) 在权值分布为D1的训练数据上,阈值v取2.5时分类误差率最低,故基本分类器为',
            'G1(x)=1 x<2.5, G1(x)=-1 x>2.5')
        print(' (b) G1(x)在训练数据集上的误误差率e1=P(G1(xi)!=yi)=0.3')
        print(' (c) 计算G1(x)的系数:a1=0.5log(1-e1)/e1=0.4236')
        print(' (d) 更新训练数据的权值分布:D2=(w21,...,w2i,...,w110)',
            'w2i=w1i/Z1exp(-a1yiG1(xi)),i=1,2,...,10')
        print('  D2=(0.0715m0.0715m0.0715,...)')
        print('  f1(x)=0.4236G1(x)')
        print('分类器sign[f1(x)]在训练数据集上有3个误分类点')
        print('对m=2,')
        print(' (a) 在权值分布为D2的训练数据上,阈值v是8.5时分类误差率最低,基本分类器为：',
            'G2(x)=1, x<8.5; G2(x)=-1, x>8.5')
        print(' (b) G2(x)z在训练数据集上的误差率e2=0.2143')
        print(' (c) 计算a2=0.6496')
        print(' (d) 更新训练数据权值分布:D3=(...),f(2)=0.4236G1(x)+0.6496G2(x); f2(x)',
            '分类器sign[f2(x)]在训练数据集上有3个误分类点')
        print('对m=3,同理可得.')
        print('8.2 AdaBoost算法的训练误差分析.')
        print('定理8.1(AdaBoost的训练误差) 训练误差界为:')
        print('  1/N∑I(G(xi)!=yi)<=1/N∑exp(-yi,f(xi))=∏Zm')
        print('这一定理说明,可以在每一轮选取适当的Gm使得Zm最小,从而使训练误差下降最快.')
        print('定理8.2(二类分类问题AdaBoost的训练误差界)')
        print('  ∏Zm=∏[2sqrt(em(1-em))]=∏sqrt(1-4ym^2)<=exp(-2∑ym^2),这里ym=0.5-em')
        print('推论8.1 如果存在y>0,对所有m有ym>=y,则1/N∑I(G(xi)!=yi)<=exp(-2My^2)')
        print('  表明AdaBoost的训练误差是以指数速率下降的.')
        print('注意:AdaBoost算法不需要知道下界y.与一些早期的提升方法不同,AdaBoost具有适应性,',
            '即它能适应弱分类器各自的训练误差率.Ada是Adaptive(适应)的简写.')
        print('8.3 AdaBoost算法的解释')
        print('AdaBoost算法还有另一解释,即可以认为AdaBoost算法是模型为加法模型、损失函数为指数函数、',
            '学习算法为前向分步算法时的二类分类学习方法.')
        print('8.3.1 前向分布算法')
        print('考虑加法模型:f(x)=∑bmb(x;ym).其中,b(x;ym)为基函数,bm为基函数的系数.',
            '显然式(8,6)是一个加法模型.')
        print('在给定训练数据及损失函数L(y,f(x))的条件下,学习加法模型f(x)成为经验风险极小化即损失函数极小化问题：',
            'min ∑L(yi,∑bmb(xi;ym))')
        print('通常这是一个复杂的优化问题.前向分布算法(forward stagewise algorithm)求解',
            '这一优化问题的想法是:因为学习的是加法模型,如果能够从前向后,每一步只学习一个基函数及其系数,',
            '逐步逼近优化目标函数式,那么就可以简化优化的复杂度.具体地,每步只需优化的复杂度.',
            '具体地,每步只需优化如下损失函数:min∑L(yi,bb(xi;y))')
        print('给定训练数据集T={(x1,y1),(x2,y2),...,(xn,yn)},xi∈X∈R^n,',
            'yi∈Y={-1,+1}.损失函数L(y,f(x));基函数集{b(x;y)};')
        print('输出：加法模型f(x).')
        print('(1) 初始化f0(x)=0')
        print('(2) 对m=1,2,...,M')
        print('  (a) 极小化损失函数(bm,ym)=argmin∑L(yi,f(m-1)(xi)+bb(xi;y))','
            '得到参数bm,ym)
        print('  (b) 更新fm(x)=f(m-1)(x)+bmb(x;ym)')
        print('(3) 得到加法模型:f(x)=fM(x)=∑bmb(x;ym)')
        print('这样,前向分布算法将同时求解从m=1到M所有参数bm,ym的优化问题.')
        print('8.3.2 前向分布算法与AdaBoost')
        print('定理8.3 AdaBoost算法是前向分布加法算法的特例.这时,模型是由基本分类器组成的加法模型,',
            '损失函数是指数函数.')
        print('证明:前向分布算法学习的是加法模型,当基函数为基本分类器时,',
            '该加法模型等价于AdaBoost的最终分类器 f(x)=∑amGm(x)')
        print('由基本分类器Gm(x)及其系数am组成,m=1,2,...,M.前向分布算法逐一学习基函数,',
            '这一过程与AdaBoost算法逐一学习基本分类器的过程一致.')
        print('前向分布算法的损失函数是指数损失函数L(y,f(x))=exp[-yf(x)]')
        print('8.4 提升树')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')
        print('')

chapter8 = Chapter8()

def main():
    chapter8.note()

if __name__ == '__main__':
    main()