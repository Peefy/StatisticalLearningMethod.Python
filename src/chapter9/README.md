# 第9章 EM算法及其推广

1.EM算法是含有隐变量的概率模型极大似然估计或极大后验概率估计的迭代算法. 含有隐变量的概率模型的数据表示为P(Y,Z|t).这里,Y是观测变量的数据,Z是隐变量的数据, t是模型参数.EM算法通过迭代求解观测数据的对数似然函数L(t)=logP(Y|t)的极大化, 实现极大似然估计.每次迭代包括两步:E步,求期望,即求logP(Y,Z|t)关于P(Z|Y,t(i))的期望: 称为Q函数,这里t(i)是参数的现估计值;M步,求极大,即极大化Q函数得到参数的新估计值: t(i+1)=argmaxQ(t,t(i))
在构建具体的EM算法时,重要的是定义Q函数.每次迭代中,EM算法通过极大化Q函数来增大对数似然函数L(t).
2.EM算法在每次迭代后均提高观测数据的似然函数值,即
  P(Y|t(i+1))>=P(Y|t(i))
在一般条件下EM算法是收敛的,但不能保证收敛到全局最优
3.EM算法应用极其广泛,主要应用于含有隐藏变量的概率模型的学习. 高斯混合模型的参数估计是EM算法的一个重要应用
4.EM算法还可以解释为F函数的极大-极大算法.EM算法有许多变形,如GEM算法. GEM算法的特点是每次迭代增加F函数值(并不一定是极大化F函数),从而增加似然函数值.


