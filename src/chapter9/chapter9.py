
class Chapter9:
    """
    第9章 EM算法及其推广
    """
    def __init__(self):
        """
        第9章 EM算法及其推广
        """
        pass

    def note(self):
        """
        chapter9 note
        """
        print('第9章 EM算法及其推广')
        print('EM算法是一种迭代算法,1977年由Dempster等人总结提出,',
            '用于含有隐变量(hidden variable)的概率模型参数的极大似然估计',
            '或极大后验概率估计.EM算法的每次迭代由两步组成:E步,求期望(expectation);M步,',
            '求极大.所以这一算法称为期望极大算法(expectation maximization algorithm),',
            '简称EM算法.EM算法的一个应用:高斯混合模型,以及EM算法的推广-GEM算法')
        print('9.1 EM算法的引入')
        print('概率模型有时含有观测变量(observable variable),又含有隐变量或潜在变量',
            '如果概率模型的变量都是观测变量,那么给定数据,可以直接用极大似然估计法,',
            '或者贝叶斯估计法估计模型参数.但是,当模型含有隐变量时,就不能简单地使用这些估计方法.',
            'EM算法就是含有隐变量的概率模型参数的极大似然估计法,或极大后验概率估计法.',
            '仅讨论极大似然估计,极大后验概率估计与其类似.')
        print('9.1.1 EM算法')
        print('例9.1 (三硬币模型)假设有3枚硬币,分别记作A,B,C.这些硬币正面出现的概率分别是pi,p和q.',
            '进行如下投掷硬币试验,先掷硬币A,根据其结果选出硬币B或硬币C;然后掷选出的硬币,掷硬币的结果,',
            '出现正面记作1,出现反面记作0;独立地重复n次试验(n=10),观测结果如下:1,1,0,1,0,1,0,1,1')
        print('假设只能观测到掷硬币的结果,不能观测掷硬币的过程.')
        print('解:三硬币模型可以写作:')
        print('  P(y|a)=∑P(y,z|a)=∑P(z|a)P(y|z,a)=pip^y(1-p)^(1-y)q^y(1-q)^(1-y)')
        print('这里,随机变量y是观测变量,表示一次试验观测的结果是1或0;随机变量z是隐藏变量,',
            '表示未观测到的掷硬币A的结果,a=(pi,p,q)是模型观测参数.',
            '这一模型是以上数据的生成模型.注意:随机变量y的数据可以观测,随机变量z的数据不可观测.')
        print('将观测数据表示为Y=(Y1,Y2,...,Yn)^T,未观测数据表示为Z=(Z1,Z2,...,Zn)^T,',
            '则观测数据的似然函数为:P(Y|a)=∑P(Z|a)P(Y|Z,a)')
        print('即P(Y|a)=∏[pip^yj(1-p)^(1-yj)+(1-pi)q^yj(1-q)^(1-yj)]')
        print('考虑求模型参数a=(pi,p,q)的极大似然估计,即a=argmaxlogP(Y|a)')
        print('EM算法首先选取参数的初值,记作a(0)=(pi(0),p(0),q(0)),然后通过下面的步骤迭代计算参数的估计值,',
            '直至收敛为止.第i次迭代参数的估计值为a(i)=(pi(i),p(i),q(i)).EM算法的第i+1次迭代如下.')
        print('E步:计算在模型参数pi(i),p(i),q(i)下观测数据yj来自掷硬币B的概率:')
        print('  计算u(i+1)')
        print('M步:计算模型参数的新估计值pi(i+1),p(i+1),q(i+1)')
        print('进行数字计算.假设模型参数的初值取为:')
        print('  pi(0)=0.5,p(0)=0.5,q(0)=0.5')
        print('对yj=1与yj=0均有uj(i)=0.5.')
        print('利用迭代公式,得到:pi(1)=0.5,p(1)=0.6,q(1)=0.6')
        print('  uj(2)=0.5,j=1,2,...,10')
        print('继续迭代,得:pi(2)=0.5,p(2)=0.6,q(2)=0.6')
        print('于是得到模型参数a的极大似然估计为:pi=0.5,p=0.6,q=0.6')
        print('pi=0.5表示硬币是均匀的.')
        print('如果选择初值pi(0)=0.4,p(0)=0.6,q(0)=0.7,那么得到的模型参数的极大似然估计是',
            'pi=0.4064,p=0.5368,q=0.6432.这就是说,EM算法与初值的选择有关,',
            '选择不同的初值可能得到不同的参数估计值')
        print('一般地,用Y表示观测随机变量的数据,Z表示隐随机变量的数据.Y和Z连在一起称为完全数据(complete-data)',
            '观测数据Y又称为不完全数据(incomplete-data).假设给定观测数据Y,其概率分布是P(Y|a),',
            '其中a是需要估计的模型参数,那么不完全数据Y的似然函数是P(Y|a),',
            '对数似然函数L(a)=logP(Y|a);假设Y和Z的联合概率分布是P(Y,Z|a),',
            '那么完全数据的对数似然函数是logP(Y,Z|a)')
        # !EM算法通过迭代求L(a)=logP(Y|a)的极大似然估计.每次迭代包含两步
        print('EM算法通过迭代求L(a)=logP(Y|a)的极大似然估计.每次迭代包含两步：',
            'E步,求期望;M步,求极大化.下面介绍EM算法')
        print('算法9.1(EM算法)')
        print('输入:观测变量数据Y,隐藏量数据Z,联合分布P(Y,Z|a),条件分布P(Z|Y,a)')
        print('输出:模型参数a.')
        print('(1) 选择参数的初值a(0),开始迭代;')
        print('(2) E步:记a(i)为第i次迭代参数a的估计值,在第i+1次迭代的E步,计算',
            'Q(a,a(i))=Ez[logP(Y,Z|a)|Y,a(i)]=∑logP(Y,Z|a)P(Z|Y,a(i))')
        print('这里,P(Z|Y,a(i))是在给定观测数据Y和当前的参数估计a(i)下隐藏变量数据Z的条件概率分布;')
        print('(3) M步:求使Q(a,a(i))极大化的a,确定第i+1次迭代的参数的估计值a(i+1)=argmaxQ(a,a(i))')
        print('(4) 重复第(2)步和第(3)步,直到收敛.')
        print('函数Q(a,a(i))=Ez[logP(Y,Z|a)|Y,a(i)]=∑logP(Y,Z|a)P(Z|Y,a(i))是EM算法的核心,称为Q函数')
        print('定义9.1(Q函数)完全数据的对数似然函数logP(Y,Z|a)关于在给定观测数据Y和当前参数a(i)',
            '下对未观测数据Z的条件概率分布P(Z|Y,a(i))的期望称为Q函数,即',
            'Q(a,a(i))=Ez[logP(Y,Z|a)|Y,a(i)]')
        print('下面关于EM算法作几点说明:')
        print('步骤(1) 参数的初值可以任意选择,但需注意EM算法对初值是敏感的.')
        print('步骤(2) E步求Q(a,a(i)).Q函数式中Z是未观测数据,Y是观测数据.',
            '注意,Q(a,a(i))的第1个变元表示要极大化的参数,第2个变元表示参数的当前估计值.',
            '每次迭代使似然函数增大或达到局部极值.')
        print('步骤(3) M步求Q(a,a(i))的极大化,得到a(i+1),完成一次迭代a(i)->a(i+1).',
            '后面将证明每次迭代使似然函数增大或达到局部极值.')
        print('步骤(4) 给出停止迭代的条件,一般是对较小的正数e1,e2,',
            '若满足||a(i+1)-a(i)||<e1或||Q(a(i+1),a(i))-Q(a(i),a(i))||<e2则停止迭代')
        print('9.1.2 EM算法的导出')
        print('面对一个含有隐藏变量的概率模型,目标是极大化观测数据(不完全数据)Y关于参数a的对数似然函数,',
            '即极大化L(a)=logP(Y|a)=log∑P(Y,Z|a)=log(∑P(Y|Z,a)P(Z|a))')
        print('这一极大化的主要困难是式子中有未观测数据并有包含和(或积分)的对数')
        print('事实上,EM算法是通过迭代逐步近似极大化L(a)的.假设在第i次迭代后a的估计值是a(i).',
            '希望新估计值a能使L(a)增加,即L(a)>L(a(i)).并逐步达到极大值.为此,考虑两者的差:')
        print('L(a)-L(a(i))=log(∑P(Y|Z,a)P(Z|a))-logP(Y|a(i))')
        print('可以利用Jensen不等式(Jensen inequality)得到其下界,')
        print('a(i+1)=argmaxQ(a,a(i))等价于EM算法的一次迭代,即求Q函数及其极大化.',
            'EM算法是通过不断求解下界的极大化逼近对数似然函数极大化的算法.')
        print('图中给出EM算法的直观解释.图中上方曲线为L(a),下方曲线为B(a,a(i)).',
            'B(a,a(i))为对数似然函数L(a)的下界,两个函数在a=a(i)处相等.',
            'EM算法找到下一个点a(i+1)使函数B(a,a(i))极大化,也使函数Q(a,a(i))极大化.',
            '这时由于L(a)>=B(a,a(i)),函数B(a,a(i))的增加,',
            '保证对数似然函数L(a)在每次迭代中也是增加的.',
            'EM算法在点a(i+1)重新计算Q函数值,进行下一次迭代.',
            '在这个过程中,对数似然函数L(a)不断增大.')
        print('结论：EM算法不能保证找到全局最优解')
        print('9.1.3 EM算法在非监督学习中的应用')
        print('监督学习是由训练数据{(x1,y1),(x2,y2),...,(xN,yN)}学习条件概率分布P(Y|X)或',
            '决策函数Y=f(X)作为模型,用于分类,回归,标注等任务.',
            '这时训练数据中的每个样本点由输入和输出对组成.')
        print('有时训练数据只有输入没有对应的输出{(x1,·),(x2,·),(x3,·),...,(xN,·)},',
            '从这样的数据学习模型称为非监督学习问题.EM算法可以用于生成模型的非监督学习.',
            '生成模型由联合概率分布P(X,Y)表示,可以认为非监督学习训练数据是联合概率分布产生的数据.',
            'X为观测数据,Y为未观测数据.')
        print('9.2 EM算法的收敛性')
        print('EM算法提供一种近似计算含有隐变量概率模型的极大似然估计的方法.',
            'EM算法的最大优点是简单性和普适性.')
        print('EM算法收敛性的两个定理')
        print('定理9.1 设P(Y|a)为观测数据的似然函数,a(i)(i=1,2,...)为EM算法得到的参数估计序列,',
            'P(Y|a(i))(i=1,2,...)为对应的似然函数序列,则P(Y|a(i))是单调递增的,即P(Y|a(i+1))>=P(Y|a(i))')
        print('定理9.2 设L(a)=logP(Y|a)为观测数据的对数似然函数,a(i)(i=1,2,...)为EM算法得到的参数估计序列,',
            'L(a(i))(i=1,2,...)为对应的对数似然函数序列.')
        print(' (1) 如果P(Y|a)有上界,则L(a(i))=logP(Y|a(i))收敛到某一值L*')
        print(' (2) 在函数Q(a,a’)与L(a)满足一定条件下,由EN算法得到的参数估计序列a(i)的收敛值a*是L(a)的稳定点')
        print('9.3 EM算法在高斯混合模型学习中的应用')
        print('EM算法的一个重要应用是高斯混合模型的参数估计.高斯混合模型应用广泛,',
            '在许多情况下,EM算法是学习高斯混合模型(Gaussian misture model)的有效方法')
        print('9.3.1 高斯混合模型')
        print('定义9.2 (高斯混合模型) 高斯混合模型是指具有如下形式的概率分布模型:P(y|a)=∑akphi(y|tk)')
        print('其中,ak>=0是系数,∑ak=1;phi(y|tk)是高斯分布密度,tk=(uk,o^2)')
        print(' phi(y|tk)=1/sqrt(2pi)·o·exp(-(y-uk)^2/2o^2)称为第k个分模型')
        print('一般混合模型可以由任意概率分布密度代替式中的高斯分布密度.')
        print('9.3.2 高斯混合模型参数估计的EM算法')
        print('假设观测数据y1,y2,...,yN由高斯混合模型生成.P(y|t)=∑akphi(y|tk)')
        print('其中,t=(a1,a2,...,aK;t1,t2,...,tK),用EM算法估计高斯混合模型的参数t')
        print('1.明确隐变量,写出完全数据的对数似然函数')
        print('  可以设想观测数据yj,j=1,2,...,N,是这样产生的:首先依概率ak萱蕚第k个高斯分布分模型phi(y|tk);',
            '然后依第k个分模型概率分布phi(y|tk)生成观测数据yj,这时观测数据yj,j=1,2,...,N,',
            '是已知的;反应观测数据yj来自第k个分模型的数据是未知的,k=1,2,...,K,以隐藏变量yjk表示,其定义如下:')
        print(' yjk=1,第j个观测来自第k个分模型; yjk=0,否则. j=1,2,...,N;k=1,2,...,K')
        print('yjk是0-1随机变量.有了观测数据yj及未观测数据yjk,那么完全数据是:')
        print('  (yj,yj1,yj2,...,yjK),j=1,2,...,N')
        print('于是,可以写出完全数据的似然函数. P(y|y|a)=∏P(yj,yj1,yj2,...,yjK|a)')
        print('那么,完全数据的对数似然函数为:')
        print('  logP(y,y|t)=∑nklogak+∑yjk[log(1/sqrt(2pi)-logok-1/(2ok^2)(yj-uk)^2)]')
        print('2.EM算法的E步：确定Q函数')
        print(' yjk是在当前模型参数下第j个观测数据来自第k个分模型的概率,',
            '称为分模型k对观测数据yj的响应度.')
        print('3.确定EM算法那的M步.')
        print('迭代的M步是求函数Q(t,t(i))对t的极大值,即求新一轮迭代的模型参数:t(i+1)=argmaxQ(t,t(i))')
        print('用uk,ok及ak,k=1,2,...,K,表示t(i+1)的各参数,求uk,ok只需将上式分别对uk,ok求偏导并令其为0,',
            '即可得到:求ak是在∑ak=1条件下求偏导并令其为0得到的.')
        print('重复以上计算,直到对数似然函数值不再有明显的变化为止')
        print('算法9.2(高斯混合模型参数估计的EM算法)')
        print('输入:观测数据y1,y2,...,yN,高斯混合模型')
        print('输出:E步,依据当前模型参数,计算分模型k对观测数据yj的响应度')
        print('(1) 取参数的初始值开始迭代')
        print('(2) E步：依据当前模型参数,计算分模型k对观测数据yj的响应度')
        print('(3) M步：计算新一轮迭代的模型参数')
        print('(4) 重复第(2)步和第(3)步,直到收敛')
        print('9.4 EM算法的推广')
        print('定义9.3（F函数）假设隐变量数据Z的概率分布为P(Z),定义分布P与参数t的函数F(P,t)如下：')
        print('  F(P,t)=Ep[logP(Y,Z|t)]+H(P) 称为F函数.式中H(P)=-EplogP(Z)是分布P(Z)的熵')
        print('在定义9.3中,通常假设P(Y,Z|t)是t的连续函数,因而F(P,t)是P和t的连续函数.函数F(P,t)还有如下重要性质:')
        print('引理9.1 对于固定的t,存在唯一的分布Pt极大化F函数,这时Pt由下式给出Pt(Z)=P(Z|Y,t),',
            '并且Pt随t连续变化.')
        print('引理9.2 若Pt(Z)=P(Z|Y,t),则F(P,t)=logP(Y|t)')
        print('由以上引理,可以得到关于EM算法用F函数的极大-极大算法的解释.')
        print('定理9.3 设L(t)=logP(Y|t)为观测数据的对数似然函数,t(i),i=1,2,...,',
            '为EM算法得到的参数估计序列,函数F(P,t)由式F(P,t)=Ep[logP(Y,Z|t)]+H(P)定义,',
            '如果F函数在Pt*和t*处有局部极大值,那么L(t)在t*处也有局部极大值.类似地,',
            '如果F函数在Pt*和t*处达到全局最大值,那么L(t)也在t*达到全局最大值.')
        print('定理9.4 EM算法的一次迭代可由F函数的极大-极大算法实现.')
        print(' 设t(i)为第i次迭代参数t的估计,P(i)为第i次迭代函数P的估计.',
            '在第i+1次迭代的两步为:')
        print(' (1) 对固定的t(i),求P(i+1)使F(P,t(i))极大化')
        print(' (2) 对固定的P(i+1),求t(i+1)使F(P(i+1),t)极大化')
        print('算法9.3 (GEM算法1)')
        print('输入:观测数据,F函数;')
        print('输出:模型参数.')
        print('(1) 初始化参数t(0),开始迭代')
        print('(2) 第i+1次迭代,第1步:记t(i)为参数t的估计值,P(i)为函数P的估计值.',
            '求P(i+1)使P极大化F(P,t(i))')
        print('(3) 第2步:求t(i+1)使F(P,t)极大化')
        print('(4) 重复(2)和(3),直到收敛')
        print('注意：在GEM算法1中,有时求Q函数的极大化是很困难的.',
            '下面介绍的GEM算法2和GEM算法3并不是直接求t(i+1)使Q函数达到极大的t',
            '而是找一个t(i+1)使得Q(t(i+1),t(i))>Q(t(i),t(i))')
        print('算法9.4 (GEM算法2)')
        print('输入:观测数据,Q函数;')
        print('输出:模型参数.')
        print('(1) 初始化参数t(i),开始迭代')
        print('(2) 第i+1次迭代,第1步,记t(i)为参数t估计值,计算')
        print('(3) 第2步:求t(i+1)使得Q(t(i+1),t(i))>Q(t(),t(i))')
        print('(4) 重复(2)和(3),直到收敛,')
        print('算法9.5 (GEM算法3)')
        print('输入:观测数据,Q函数')
        print('输出:模型参数')
        print('(1) 初始化参数t(0)=(t1(0),t2(0),...,td(0)),开始迭代')
        print('(2) 第i+1次迭代,第1步:记t(i)=(t1(i),t2(i),...,td(i))为参数t=(t1,t2,...,td)的估计值,计算',
            'Q(t,t(i))=Ez[logP(Y,Z|t)|Y,t(i)]=∑P(Z|y,t(i))logP(Y,Z|t)')
        print('(3) 第2步：进行d次条件极大化.')
        print('首先,在t2(i),...,tk(i)保持不变的条件下求使得Q(t,t(i))达到极大的t1(i+1)')
        print('然后,在t1=t1(i+1),tj=tj(i),j=3,4,...,k的条件下求使Q(t,t(i))达到极大的t2(i+1)')
        print('如此继续,经过d次条件极大化,得到t(i+1)=(t1(i+1),t2(i+1),...,td(i+1))',
            '使得Q(t(i+1),t(i))>Q(t(i),t(i))')
        print('(4) 重复(2)和(3),直到收敛.')
        print('本章概要')
        print('1.EM算法是含有隐变量的概率模型极大似然估计或极大后验概率估计的迭代算法.',
            '含有隐变量的概率模型的数据表示为P(Y,Z|t).这里,Y是观测变量的数据,Z是隐变量的数据,',
            't是模型参数.EM算法通过迭代求解观测数据的对数似然函数L(t)=logP(Y|t)的极大化,',
            '实现极大似然估计.每次迭代包括两步:E步,求期望,即求logP(Y,Z|t)关于P(Z|Y,t(i))的期望:',
            '称为Q函数,这里t(i)是参数的现估计值;M步,求极大,即极大化Q函数得到参数的新估计值:',
            't(i+1)=argmaxQ(t,t(i))')
        print('在构建具体的EM算法时,重要的是定义Q函数.每次迭代中,EM算法通过极大化Q函数来增大对数似然函数L(t).')
        print('2.EM算法在每次迭代后均提高观测数据的似然函数值,即')
        print('  P(Y|t(i+1))>=P(Y|t(i))')
        print('在一般条件下EM算法是收敛的,但不能保证收敛到全局最优')
        print('3.EM算法应用极其广泛,主要应用于含有隐藏变量的概率模型的学习.',
            '高斯混合模型的参数估计是EM算法的一个重要应用')
        print('4.EM算法还可以解释为F函数的极大-极大算法.EM算法有许多变形,如GEM算法.',
            'GEM算法的特点是每次迭代增加F函数值(并不一定是极大化F函数),从而增加似然函数值.')
  
chapter9 = Chapter9()

def main():
    chapter9.note()

if __name__ == '__main__':
    main()